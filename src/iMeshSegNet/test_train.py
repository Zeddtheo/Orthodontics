#!/usr/bin/env python3
"""MeshSegNetè®­ç»ƒæµ‹è¯•è„šæœ¬"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from m1_train import TrainConfig, Trainer, build_ce_class_weights
from m0_dataset import DataConfig, get_dataloaders, load_split_lists, compute_label_histogram, set_seed, SEG_NUM_CLASSES
from imeshsegnet import iMeshSegNet
import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR

def quick_train():
    """å¿«é€Ÿæµ‹è¯•è®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹MeshSegNetå¿«é€Ÿæµ‹è¯•è®­ç»ƒ...")
    
    # é…ç½®
    config = TrainConfig()
    config.epochs = 5  # å¿«é€Ÿæµ‹è¯•
    config.output_dir = Path('../../test_runs/mesh_train_test')
    config.data_config.dataset_root = Path('../../datasets/segmentation_dataset')
    config.data_config.split_path = Path('../../outputs/segmentation/module0/dataset_split.json')
    config.data_config.stats_path = Path('../../outputs/segmentation/module0/stats.npz')
    config.data_config.batch_size = 4  # å°æ‰¹æ¬¡
    config.data_config.num_workers = 0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    
    set_seed(42)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®å‡†å¤‡
    config.data_config.pin_memory = device.type == "cuda"
    config.data_config.persistent_workers = False  # é¿å…workeré—®é¢˜
    config.data_config.augment = True
    
    # è®¡ç®—ç±»æƒé‡
    if config.ce_class_weights is None:
        print("è®¡ç®—ç±»é¢‘ç‡æƒé‡...")
        train_files, _ = load_split_lists(config.data_config.split_path)
        class_hist = compute_label_histogram(train_files)
        ce_weights = build_ce_class_weights(class_hist)
        config.ce_class_weights = ce_weights.tolist()
        print(f"CrossEntropyç±»æƒé‡: min={ce_weights.min():.3f}, max={ce_weights.max():.3f}")
    
    print("åŠ è½½æ•°æ®é›†...")
    train_loader, val_loader = get_dataloaders(config.data_config)
    print(f"è®­ç»ƒé›†: {len(train_loader)} æ‰¹æ¬¡")
    print(f"éªŒè¯é›†: {len(val_loader)} æ‰¹æ¬¡")
    
    # æ¨¡å‹
    print("åˆå§‹åŒ–æ¨¡å‹...")
    model = iMeshSegNet(
        num_classes=config.num_classes,
        glm_impl="edgeconv",
        k_short=6,
        k_long=12,
        with_dropout=True,
        dropout_p=0.5,
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°: {total_params:,} æ€»è®¡, {trainable_params:,} å¯è®­ç»ƒå‚æ•°")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = Adam(
        model.parameters(),
        lr=config.lr,
        amsgrad=True,
        weight_decay=config.weight_decay,
    )
    scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=config.min_lr)
    
    # è®­ç»ƒå™¨
    trainer = Trainer(model, train_loader, val_loader, optimizer, scheduler, config, device)
    trainer.train()
    
    print("âœ… MeshSegNetå¿«é€Ÿæµ‹è¯•è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    quick_train()