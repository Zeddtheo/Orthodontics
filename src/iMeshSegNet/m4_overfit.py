#!/usr/bin/env python3
"""
m5_overfit.py - å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒè„šæœ¬
ç”¨äºŽéªŒè¯æ¨¡åž‹å’Œè®­ç»ƒç®¡é“çš„æ­£ç¡®æ€§ï¼Œé€šè¿‡åœ¨å•ä¸ªæ ·æœ¬ä¸Šè¿‡æ‹Ÿåˆæ¥å¿«é€Ÿè¯Šæ–­é—®é¢˜ã€‚

ä½¿ç”¨æ–¹æ³•:
    python m5_overfit.py --sample 1_L  # å¯¹1_L.vtpè¿›è¡Œè¿‡æ‹Ÿåˆ
    python m5_overfit.py --sample 5_U --epochs 500  # è‡ªå®šä¹‰epochæ•°
"""

import argparse
import sys
from pathlib import Path
from typing import List, Tuple
import time

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.amp import autocast, GradScaler
from torch.nn.utils import clip_grad_norm_
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import pyvista as pv
from sklearn.neighbors import NearestNeighbors, KDTree

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent.parent))

from m0_dataset import (
    DECIM_CACHE,
    FDI_LABELS,
    LABEL_REMAP,
    SEG_NUM_CLASSES,
    SINGLE_ARCH_NUM_CLASSES,
    _build_single_arch_label_maps,
    _load_or_build_decimated_mm,
    extract_features,
    find_label_array,
    normalize_mesh_units,
    remap_labels_single_arch,
    remap_segmentation_labels,
)
from m1_train import GeneralizedDiceLoss
from imeshsegnet import iMeshSegNet


def calculate_metrics(preds: torch.Tensor, labels: torch.Tensor, num_classes: int) -> Tuple[float, float, float]:
    """
    è®¡ç®— DSC, Sensitivity, PPV
    
    Args:
        preds: é¢„æµ‹æ ‡ç­¾ (N,)
        labels: çœŸå®žæ ‡ç­¾ (N,)
        num_classes: ç±»åˆ«æ•°
        
    Returns:
        (dsc, sensitivity, ppv)
    """
    dsc_list = []
    sen_list = []
    ppv_list = []
    
    for cls in range(1, num_classes):  # è·³è¿‡èƒŒæ™¯ç±»
        pred_mask = (preds == cls)
        label_mask = (labels == cls)
        
        tp = (pred_mask & label_mask).sum().item()
        fp = (pred_mask & ~label_mask).sum().item()
        fn = (~pred_mask & label_mask).sum().item()
        
        if tp + fp + fn == 0:
            continue
            
        dsc = 2 * tp / (2 * tp + fp + fn + 1e-8)
        sen = tp / (tp + fn + 1e-8)
        ppv_val = tp / (tp + fp + 1e-8)
        
        dsc_list.append(dsc)
        sen_list.append(sen)
        ppv_list.append(ppv_val)
    
    return (
        float(np.mean(dsc_list)) if dsc_list else 0.0,
        float(np.mean(sen_list)) if sen_list else 0.0,
        float(np.mean(ppv_list)) if ppv_list else 0.0
    )


def _build_neighbors(pos: np.ndarray, k: int = 12) -> np.ndarray:
    n = pos.shape[0]
    if n <= 1:
        return np.zeros((n, 0), dtype=np.int32)
    k_eff = max(1, min(k, n - 1))
    nn = NearestNeighbors(n_neighbors=k_eff, algorithm="auto")
    nn.fit(pos)
    neighbors = nn.kneighbors(pos, return_distance=False)
    idx = np.tile(np.arange(n)[:, None], (1, k_eff))
    neighbors = np.where(neighbors != idx, neighbors, -1)
    return neighbors.astype(np.int32, copy=False)


def _boundary_flags(labels: np.ndarray, neighbors: np.ndarray) -> np.ndarray:
    boundary = np.zeros(labels.shape[0], dtype=bool)
    for idx, nbrs in enumerate(neighbors):
        for nbr in nbrs:
            if nbr < 0:
                continue
            if labels[nbr] != labels[idx]:
                boundary[idx] = True
                break
    return boundary


def compute_boundary_metrics(
    pos_mm: np.ndarray,
    gt_labels: np.ndarray,
    pred_labels: np.ndarray,
    *,
    gingiva_label: int = 15,
    tau: float = 0.2,
    neighbor_k: int = 12,
) -> Tuple[float, float, float, float, float]:
    neighbors = _build_neighbors(pos_mm, neighbor_k)
    gt_boundary = _boundary_flags(gt_labels, neighbors)
    pred_boundary = _boundary_flags(pred_labels, neighbors)

    precision = recall = bf1 = 0.0
    if np.any(pred_boundary) and np.any(gt_boundary):
        tree_gt = KDTree(pos_mm[gt_boundary])
        dist_pred, _ = tree_gt.query(pos_mm[pred_boundary], k=1, return_distance=True)
        precision = float(np.mean(dist_pred <= tau)) if dist_pred.size else 0.0

        tree_pred = KDTree(pos_mm[pred_boundary])
        dist_gt, _ = tree_pred.query(pos_mm[gt_boundary], k=1, return_distance=True)
        recall = float(np.mean(dist_gt <= tau)) if dist_gt.size else 0.0

        if precision + recall > 0:
            bf1 = 2 * precision * recall / (precision + recall)

    gingival_mask = gt_boundary & (gt_labels != gingiva_label)
    ger = 0.0
    if np.any(gingival_mask):
        ger = float(np.mean(pred_labels[gingival_mask] == gingiva_label))

    leak_mask = np.zeros(gt_labels.shape[0], dtype=bool)
    leak_target = np.zeros(gt_labels.shape[0], dtype=np.int32)
    for idx, nbrs in enumerate(neighbors):
        base = gt_labels[idx]
        if base <= 0 or base == gingiva_label:
            continue
        for nbr in nbrs:
            if nbr < 0:
                continue
            nb_lbl = gt_labels[nbr]
            if nb_lbl > 0 and nb_lbl != gingiva_label and nb_lbl != base:
                leak_mask[idx] = True
                leak_target[idx] = nb_lbl
                break
    ilr = 0.0
    if np.any(leak_mask):
        ilr = float(np.mean(pred_labels[leak_mask] == leak_target[leak_mask]))

    return bf1, precision, recall, ger, ilr


DEFAULT_LABEL_MODE = "single_arch_16"
DEFAULT_GINGIVA_SRC = 0
DEFAULT_GINGIVA_CLASS_ID = 15
DEFAULT_KEEP_VOID_ZERO = False
FDI_LABEL_SET = set(FDI_LABELS)
INV_LABEL_REMAP = {v: k for k, v in LABEL_REMAP.items()}


class SingleSampleDataset(Dataset):
    """å•æ ·æœ¬æ•°æ®é›† - ç”Ÿæˆä¸€æ¬¡æ ·æœ¬åŽåå¤è¿”å›ž"""

    def __init__(
        self,
        sample_file: str,
        mean: np.ndarray,
        std: np.ndarray,
        *,
        label_mode: str = DEFAULT_LABEL_MODE,
        gingiva_src_label: int = DEFAULT_GINGIVA_SRC,
        gingiva_class_id: int = DEFAULT_GINGIVA_CLASS_ID,
        keep_void_zero: bool = DEFAULT_KEEP_VOID_ZERO,
    ):
        target_cells = 10000
        sample_cells = 6000
        sample_path = Path(sample_file)

        decimated_mesh = _load_or_build_decimated_mm(sample_path, target_cells=target_cells)
        self.decim_cache_vtp = (DECIM_CACHE / f"{sample_path.stem}.c{target_cells}.vtp").resolve()

        mesh = decimated_mesh.copy(deep=True)
        mesh.points -= mesh.center

        dec_label_info = find_label_array(mesh)
        if dec_label_info is None:
            raise RuntimeError(f"{self.decim_cache_vtp} ç¼ºå¤± Label æ•°ç»„")
        _, dec_labels = dec_label_info
        dec_labels = np.asarray(dec_labels, dtype=np.int64)

        full_mesh = pv.read(str(sample_path))
        full_label_info = find_label_array(full_mesh)
        if full_label_info is None:
            raise RuntimeError(f"{sample_file} ç¼ºå¤± Label æ•°ç»„")
        _, full_raw_labels = full_label_info
        full_raw_labels = np.asarray(full_raw_labels, dtype=np.int64, copy=False)

        orig_ids = mesh.cell_data.get("vtkOriginalCellIds")
        if orig_ids is not None and full_raw_labels.size > 0:
            orig_ids_np = np.asarray(orig_ids, dtype=np.int64, copy=False)
            base_labels = full_raw_labels[orig_ids_np]
        else:
            base_labels = dec_labels

        self.label_mode = label_mode
        self._label_kwargs = {
            "gingiva_src_label": gingiva_src_label,
            "gingiva_class_id": gingiva_class_id,
            "keep_void_zero": keep_void_zero,
        }
        debug_info = {
            "raw_unique": np.unique(base_labels).astype(int).tolist(),
            "intermediate_unique": None,
            "fallback_values": [],
        }
        if label_mode == "single_arch_16":
            raw_unique = np.unique(base_labels)
            needs_inverse = any(
                (val not in FDI_LABEL_SET) and (val != gingiva_src_label) and (val != 0)
                for val in raw_unique
            )
            if needs_inverse:
                base_labels = np.asarray(
                    [INV_LABEL_REMAP.get(int(v), 0) for v in base_labels],
                    dtype=np.int64,
                )
            debug_info["intermediate_unique"] = np.unique(base_labels).astype(int).tolist()
        if label_mode == "single_arch_16":
            single_arch_maps = _build_single_arch_label_maps(
                gingiva_src_label,
                gingiva_class_id,
                keep_void_zero,
            )
            labels = remap_labels_single_arch(base_labels, sample_path, single_arch_maps)
            self.num_classes_full = SINGLE_ARCH_NUM_CLASSES
            debug_info["post_unique"] = np.unique(labels).astype(int).tolist()
        elif label_mode == "full_fdi":
            labels = remap_segmentation_labels(base_labels)
            self.num_classes_full = SEG_NUM_CLASSES
            debug_info["post_unique"] = np.unique(labels).astype(int).tolist()
        else:
            raise ValueError(f"Unsupported label_mode: {label_mode}")

        mesh, _, _, diag_after = normalize_mesh_units(mesh)
        mesh = mesh.triangulate()

        labels_full_unique = np.unique(labels).astype(np.int64, copy=False)
        self.labels_full_unique = labels_full_unique

        feats = extract_features(mesh).astype(np.float32, copy=False)
        pos_mm = mesh.cell_centers().points.astype(np.float32)
        pos_scale = diag_after if diag_after > 1e-6 else 1.0
        pos_norm = pos_mm / pos_scale

        rng = np.random.default_rng(42)
        if feats.shape[0] > sample_cells:
            total_cells = feats.shape[0]
            min_samples_per_class = 160
            required_indices: List[int] = []
            unique_labels = np.unique(labels)
            for cls in unique_labels:
                if cls == 0:
                    continue
                cls_indices = np.where(labels == cls)[0]
                if cls_indices.size == 0:
                    continue
                quota = min(min_samples_per_class, sample_cells)
                take = min(cls_indices.size, quota)
                chosen = rng.choice(cls_indices, size=take, replace=cls_indices.size < take)
                required_indices.extend(chosen.tolist())

            if required_indices:
                required_array = np.unique(np.asarray(required_indices, dtype=np.int64))
            else:
                required_array = np.empty(0, dtype=np.int64)

            slots_left = sample_cells - required_array.size
            if slots_left < 0:
                idx = rng.choice(required_array, size=sample_cells, replace=False)
            else:
                mask = np.ones(total_cells, dtype=bool)
                if required_array.size > 0:
                    mask[required_array] = False
                pool = np.nonzero(mask)[0]
                extra_take = min(slots_left, pool.size)
                extra = rng.choice(pool, size=extra_take, replace=False) if extra_take > 0 else np.empty(0, dtype=np.int64)
                if required_array.size > 0:
                    idx = np.concatenate([required_array, extra])
                else:
                    idx = extra
                if idx.size < sample_cells:
                    deficit = sample_cells - idx.size
                    fallback = rng.choice(total_cells, size=deficit, replace=True)
                    idx = np.concatenate([idx, fallback])
            idx = idx[:sample_cells]
            rng.shuffle(idx)
            feats = feats[idx]
            pos_mm = pos_mm[idx]
            pos_norm = pos_norm[idx]
            labels = labels[idx]
        else:
            idx = np.arange(feats.shape[0], dtype=np.int64)

        self.sample_indices = idx.astype(np.int64, copy=False)

        feats_norm = (feats - mean.reshape(1, -1)) / np.clip(std.reshape(1, -1), 1e-6, None)
        features_tensor = torch.from_numpy(feats_norm.astype(np.float32)).transpose(0, 1).contiguous()
        pos_tensor = torch.from_numpy(pos_norm.astype(np.float32)).transpose(0, 1).contiguous()
        pos_mm_tensor = torch.from_numpy(pos_mm.astype(np.float32)).transpose(0, 1).contiguous()
        pos_scale_tensor = torch.tensor([pos_scale], dtype=torch.float32)
        labels_tensor = torch.from_numpy(labels.astype(np.int64))

        self.sample_data = ((features_tensor, pos_tensor, pos_mm_tensor, pos_scale_tensor), labels_tensor)
        self.feature_dim = int(features_tensor.shape[0])
        self.remap_debug = debug_info

    def __len__(self) -> int:
        return 10

    def __getitem__(self, idx):
        return self.sample_data


def setup_single_sample_training(sample_name: str, dataset_root: Path) -> Tuple[DataLoader, int, np.ndarray, np.ndarray]:
    """
    è®¾ç½®å•æ ·æœ¬è®­ç»ƒæ•°æ®
    
    Args:
        sample_name: æ ·æœ¬åç§°ï¼Œå¦‚ "1_L" æˆ– "5_U"
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        
    Returns:
        (dataloader, num_classes, mean, std): æ•°æ®åŠ è½½å™¨ã€ç±»åˆ«æ•°å’Œæ ‡å‡†åŒ–å‚æ•°
    """
    print(f"ðŸ” è®¾ç½®å•æ ·æœ¬è®­ç»ƒ: {sample_name}")
    
    # æž„å»ºæ ·æœ¬æ–‡ä»¶è·¯å¾„
    sample_file = dataset_root / f"{sample_name}.vtp"
    if not sample_file.exists():
        raise FileNotFoundError(f"æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {sample_file}")
    
    # åŠ è½½ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜å…ˆå¤ç”¨ç¼“å­˜ï¼Œç¼ºå¤±åˆ™å°±åœ°è®¡ç®—ï¼‰
    stats_path = Path("outputs/segmentation/overfit/_stats_sample.npz")
    need_recompute = True
    if stats_path.exists():
        with np.load(stats_path) as stats:
            mean = stats["mean"].astype(np.float32, copy=False)
            std = stats["std"].astype(np.float32, copy=False)
        std = np.clip(std, 1e-6, None)
        feat_dim_est = extract_features(pv.read(str(sample_file)).triangulate()).shape[1]
        if mean.shape[0] == feat_dim_est:
            need_recompute = False
            print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»Ÿè®¡: {stats_path}")
        else:
            print(f"âš ï¸ ç»Ÿè®¡ç»´åº¦ä¸åŒ¹é… ({mean.shape[0]} -> {feat_dim_est})ï¼Œé‡æ–°è®¡ç®—")
    if need_recompute:
        mesh_mm = _load_or_build_decimated_mm(sample_file, target_cells=10000)
        mesh = mesh_mm.copy(deep=True)
        mesh.points -= mesh.center
        mesh, *_ = normalize_mesh_units(mesh)
        mesh = mesh.triangulate()
        feats = extract_features(mesh).astype(np.float32, copy=False)
        mean = feats.mean(axis=0).astype(np.float32, copy=False)
        std = np.clip(feats.std(axis=0), 1e-6, None).astype(np.float32, copy=False)
        stats_path.parent.mkdir(parents=True, exist_ok=True)
        np.savez(stats_path, mean=mean, std=std)
        print(f"[overfit] built sample stats -> {stats_path}")

    # åˆ›å»ºå•æ ·æœ¬æ•°æ®é›†
    single_dataset = SingleSampleDataset(
        str(sample_file),
        mean,
        std,
        label_mode=DEFAULT_LABEL_MODE,
        gingiva_src_label=DEFAULT_GINGIVA_SRC,
        gingiva_class_id=DEFAULT_GINGIVA_CLASS_ID,
        keep_void_zero=DEFAULT_KEEP_VOID_ZERO,
    )
    remap_debug = getattr(single_dataset, "remap_debug", None)

    # èŽ·å–ä¸€ä¸ªæ ·æœ¬æ¥ç¡®å®šç±»åˆ«æ•°
    sample_data = single_dataset[0]
    # æ•°æ®æ ¼å¼: ((features, pos, pos_mm, pos_scale), labels)
    (features, pos, pos_mm, pos_scale), labels = sample_data
    
    # ðŸ’¾ ä¿å­˜è®­ç»ƒä¾§æ•°ç»„ï¼ˆç”¨äºŽä¸ŽæŽ¨ç†å¯¹æ¯”ï¼‰
    # æ³¨æ„ï¼šfeatures å’Œ pos æ ¼å¼æ˜¯ (C, N)ï¼Œéœ€è¦è½¬ç½®ä¸º (N, C) ä»¥ä¾¿å¯¹æ¯”
    train_arrays_path = Path("outputs/segmentation/overfit/_train_arrays.npz")
    train_arrays_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(str(train_arrays_path),
             feats=features.transpose(0, 1).numpy(),  # (15, N) -> (N, 15)
             pos=pos.transpose(0, 1).numpy())          # (3, N) -> (N, 3)
    print(f"ðŸ’¾ ä¿å­˜è®­ç»ƒä¾§æ•°ç»„: {train_arrays_path} (shape: feats={features.shape}->{features.transpose(0,1).shape}, pos={pos.shape}->{pos.transpose(0,1).shape})")
    single_dataset.train_arrays_path = train_arrays_path.resolve()
    
    # ðŸ”¬ ä¿å­˜è®­ç»ƒæ—¶çš„é‡‡æ ·ç´¢å¼•ï¼ˆç”¨äºŽæŽ¨ç†ç«¯å¤ç”¨ï¼‰
    if hasattr(single_dataset, "sample_indices"):
        train_ids_path = Path("outputs/segmentation/overfit/_train_ids.npy")
        np.save(str(train_ids_path), np.asarray(single_dataset.sample_indices, dtype=np.int64))
        print(f"ðŸ’¾ ä¿å­˜è®­ç»ƒé‡‡æ ·ç´¢å¼•: {train_ids_path} (shape: {np.asarray(single_dataset.sample_indices).shape})")
        single_dataset.train_ids_path = train_ids_path.resolve()
    
    unique_labels = torch.unique(labels)
    default_classes = SINGLE_ARCH_NUM_CLASSES if single_dataset.label_mode == "single_arch_16" else SEG_NUM_CLASSES
    num_classes = int(getattr(single_dataset, "num_classes_full", default_classes))
    labels_full_unique = getattr(single_dataset, "labels_full_unique", None)

    # åˆ›å»ºDataLoader
    # âš¡ ä¼˜åŒ–ï¼šbatch_size=2 é¿å… BatchNorm batch_size=1 é—®é¢˜ï¼ŒåŒæ—¶åŠ é€Ÿè®­ç»ƒ
    dataloader = DataLoader(
        single_dataset,
        batch_size=2,
        shuffle=False,  # å•æ ·æœ¬ä¸éœ€è¦shuffle
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"âœ… å•æ ·æœ¬æ•°æ®é›†è®¾ç½®å®Œæˆ")
    print(f"   - æ ·æœ¬å½¢çŠ¶: features={features.shape}, pos={pos.shape}, labels={labels.shape}")
    print(f"   - æ ‡ç­¾æ¨¡å¼: {single_dataset.label_mode}")
    if remap_debug is not None:
        print(f"   - åŽŸå§‹æ ‡ç­¾ unique: {remap_debug.get('raw_unique')}")
        if remap_debug.get('intermediate_unique') is not None:
            print(f"   - è½¬æ¢åŽæ ‡ç­¾ unique: {remap_debug.get('intermediate_unique')}")
    if labels_full_unique is not None:
        print(f"   - å…¨10kå”¯ä¸€æ ‡ç­¾: {labels_full_unique.tolist()}")
    print(f"   - 6kå”¯ä¸€æ ‡ç­¾: {unique_labels.tolist()}")
    print(f"   - ç±»åˆ«æ•°: {num_classes}")
    
    return dataloader, num_classes, mean, std


class OverfitTrainer:
    """å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, dataloader: DataLoader, num_classes: int, device: torch.device, 
                 mean: np.ndarray = None, std: np.ndarray = None):
        self.model = model.to(device)
        self.dataloader = dataloader
        self.num_classes = num_classes
        self.device = device
        self.mean = mean
        self.std = std
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        self.dice_loss = GeneralizedDiceLoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
        # è®¾ç½®ä¼˜åŒ–å™¨ - ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ çŽ‡è¿›è¡Œå¿«é€Ÿè¿‡æ‹Ÿåˆ
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=0.01,  # æ¯”æ­£å¸¸è®­ç»ƒå¤§10å€
            weight_decay=0.0  # ç§»é™¤æƒé‡è¡°å‡ä»¥ä¾¿è¿‡æ‹Ÿåˆ
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        device_type = 'cuda' if device.type == 'cuda' else 'cpu'
        self.scaler = GradScaler(device_type) if device.type == 'cuda' else None
        self.use_amp = device.type == 'cuda'
        self.device_type = device_type  # ä¿å­˜ç”¨äºŽ autocast
        
        # è®°å½•è®­ç»ƒåŽ†å²
        self.history = {
            'loss': [],
            'dice_loss': [],
            'ce_loss': [],
            'dsc': [],
            'accuracy': [],
            'train_bg0': [],  # è®­ç»ƒé›†èƒŒæ™¯æ¯”ä¾‹
            'train_entropy': [],  # è®­ç»ƒé›†é¢„æµ‹ç†µ
            'val_bg0': [],  # éªŒè¯é›†èƒŒæ™¯æ¯”ä¾‹
            'val_entropy': [],  # éªŒè¯é›†é¢„æµ‹ç†µ
            'bf1': [],
            'ger': [],
            'ilr': []
        }
        
    def train_epoch(self) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        # âš¡ batch_size=2ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç† BatchNorm
        self.model.train()
        epoch_metrics = {
            'loss': 0.0,
            'dice_loss': 0.0,
            'ce_loss': 0.0,
            'correct': 0,
            'total': 0
        }
        
        for batch_idx, batch_data in enumerate(self.dataloader):
            # è§£æžbatchæ•°æ®: ((features, pos, pos_mm, pos_scale), labels)
            (features, pos, pos_mm, pos_scale), labels = batch_data
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            features = features.to(self.device, non_blocking=True)
            pos = pos.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad(set_to_none=True)
            
            if self.use_amp:
                with autocast(self.device_type):
                    logits = self.model(features, pos)
                    dice_loss = self.dice_loss(logits, labels)
                    ce_loss = self.ce_loss(logits, labels)
                    total_loss = dice_loss + ce_loss
                
                assert self.scaler is not None
                self.scaler.scale(total_loss).backward()
                self.scaler.unscale_(self.optimizer)
                clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(features, pos)
                dice_loss = self.dice_loss(logits, labels)
                ce_loss = self.ce_loss(logits, labels)
                total_loss = dice_loss + ce_loss
                
                total_loss.backward()
                clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
            
            # è®¡ç®—å‡†ç¡®çŽ‡
            preds = torch.argmax(logits, dim=1)
            correct = (preds == labels).sum().item()
            total = labels.numel()
            
            # ðŸ” æ–°å¢žè¯Šæ–­æŒ‡æ ‡ï¼šBG0 å’Œ Entropy
            with torch.no_grad():
                probs = torch.softmax(logits.detach(), dim=1)  # (B, C, N)
                # BG0: èƒŒæ™¯ç±»ï¼ˆç±»åˆ«0ï¼‰çš„é¢„æµ‹æ¯”ä¾‹
                bg_ratio = (preds == 0).float().mean().item()
                # Entropy: é¢„æµ‹ç†µï¼ˆè¡¡é‡ä¸ç¡®å®šæ€§ï¼‰
                entropy = -(probs * torch.log(probs.clamp(min=1e-8))).sum(dim=1).mean().item()
            
            if 'bg_ratio' not in epoch_metrics:
                epoch_metrics['bg_ratio'] = 0.0
                epoch_metrics['entropy'] = 0.0
            
            # ç´¯ç§¯æŒ‡æ ‡
            epoch_metrics['loss'] += total_loss.item()
            epoch_metrics['dice_loss'] += dice_loss.item()
            epoch_metrics['ce_loss'] += ce_loss.item()
            epoch_metrics['correct'] += correct
            epoch_metrics['total'] += total
            epoch_metrics['bg_ratio'] += bg_ratio
            epoch_metrics['entropy'] += entropy
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        num_batches = len(self.dataloader)
        epoch_metrics['loss'] /= num_batches
        epoch_metrics['dice_loss'] /= num_batches
        epoch_metrics['ce_loss'] /= num_batches
        epoch_metrics['accuracy'] = epoch_metrics['correct'] / epoch_metrics['total']
        epoch_metrics['bg_ratio'] /= num_batches
        epoch_metrics['entropy'] /= num_batches
        
        return epoch_metrics
    
    def evaluate(self) -> dict:
        """è¯„ä¼°æ¨¡åž‹"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_bg_ratios = []
        all_entropy = []
        pos_mm_np: Optional[np.ndarray] = None
        
        with torch.no_grad():
            for batch_data in self.dataloader:
                # è§£æžbatchæ•°æ®: ((features, pos, pos_mm, pos_scale), labels)
                (features, pos, pos_mm, pos_scale), labels = batch_data
                
                features = features.to(self.device, non_blocking=True)
                pos = pos.to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                if pos_mm_np is None:
                    pos_mm_np = pos_mm[0].transpose(0, 1).cpu().numpy().astype(np.float32, copy=False)
                
                if self.use_amp:
                    with autocast(self.device_type):
                        logits = self.model(features, pos)
                else:
                    logits = self.model(features, pos)
                
                preds = torch.argmax(logits, dim=1)
                probs = torch.softmax(logits, dim=1)
                
                # è®¡ç®—è¯Šæ–­æŒ‡æ ‡
                bg_ratio = (preds == 0).float().mean().item()
                entropy = -(probs * torch.log(probs.clamp(min=1e-8))).sum(dim=1).mean().item()
                
                all_preds.append(preds.cpu())
                all_labels.append(labels.cpu())
                all_bg_ratios.append(bg_ratio)
                all_entropy.append(entropy)
        
        # è®¡ç®—DSC
        preds_tensor = torch.cat(all_preds)
        labels_tensor = torch.cat(all_labels)
        dsc, sen, ppv = calculate_metrics(preds_tensor, labels_tensor, self.num_classes)
        bf1 = precision = recall = ger = ilr = 0.0
        if pos_mm_np is not None and preds_tensor.shape[0] > 0:
            bf1, precision, recall, ger, ilr = compute_boundary_metrics(
                pos_mm_np,
                labels_tensor[0].numpy(),
                preds_tensor[0].numpy(),
                gingiva_label=DEFAULT_GINGIVA_CLASS_ID,
            )
        
        return {
            'dsc': dsc, 
            'sensitivity': sen, 
            'ppv': ppv,
            'bg_ratio': np.mean(all_bg_ratios),
            'entropy': np.mean(all_entropy),
            'bf1': bf1,
            'ger': ger,
            'ilr': ilr,
            'boundary_precision': precision,
            'boundary_recall': recall,
        }
    
    def _save_training_evidence(self, save_dir: Path, sample_name: str):
        """
        ðŸ”¬ å†³ç­–æ ‘èŠ‚ç‚¹1ï¼šä¿å­˜è®­ç»ƒè¯æ®ï¼ˆlogits, labels, metricsï¼‰
        
        ç”¨äºŽ --replay-train æ¨¡å¼ä¸‹éªŒè¯æŽ¨ç†æ˜¯å¦å®Œå…¨å¤çŽ°è®­ç»ƒå‰å‘
        """
        import json
        
        self.model.eval()
        all_logits = []
        all_labels = []
        all_preds = []
        
        print("   ðŸ“Š æ”¶é›†æœ€ç»ˆ epoch çš„ logits å’Œ labels...")
        
        with torch.no_grad():
            for batch_data in self.dataloader:
                (features, pos, pos_mm, pos_scale), labels = batch_data
                features = features.to(self.device, non_blocking=True)
                pos = pos.to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                logits = self.model(features, pos)  # (B, C, N)
                preds = torch.argmax(logits, dim=1)  # (B, N)
                
                all_logits.append(logits.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
                all_preds.append(preds.cpu().numpy())
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        all_logits = np.concatenate(all_logits, axis=0)  # (B, C, N)
        all_labels = np.concatenate(all_labels, axis=0)  # (B, N)
        all_preds = np.concatenate(all_preds, axis=0)    # (B, N)
        
        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆå•æ ·æœ¬è®­ç»ƒï¼‰
        train_logits = all_logits[0]  # (C, N)
        train_labels = all_labels[0]  # (N,)
        train_preds = all_preds[0]    # (N,)
        
        # ä¿å­˜ logits å’Œ labels
        logits_path = save_dir.parent / "_train_logits.npy"
        labels_path = save_dir.parent / "_train_labels.npy"
        np.save(str(logits_path), train_logits)
        np.save(str(labels_path), train_labels)
        print(f"   ðŸ’¾ _train_logits.npy (shape: {train_logits.shape})")
        print(f"   ðŸ’¾ _train_labels.npy (shape: {train_labels.shape})")
        
        # è®¡ç®—å¹¶ä¿å­˜ metrics
        dsc, sen, ppv = calculate_metrics(
            torch.from_numpy(train_preds),
            torch.from_numpy(train_labels),
            self.num_classes
        )
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        conf_matrix = np.zeros((self.num_classes, self.num_classes), dtype=np.int64)
        for true_label in range(self.num_classes):
            for pred_label in range(self.num_classes):
                conf_matrix[true_label, pred_label] = np.sum(
                    (train_labels == true_label) & (train_preds == pred_label)
                )
        
        # è®¡ç®— margin (ç½®ä¿¡åº¦)
        probs = np.exp(train_logits) / np.exp(train_logits).sum(axis=0, keepdims=True)  # Softmax
        sorted_probs = np.sort(probs, axis=0)
        margins = sorted_probs[-1, :] - sorted_probs[-2, :]  # p_max - p_second
        
        metrics = {
            "sample_name": sample_name,
            "num_classes": int(self.num_classes),
            "num_cells": int(train_labels.shape[0]),
            "dsc": float(dsc),
            "sensitivity": float(sen),
            "ppv": float(ppv),
            "accuracy": float((train_preds == train_labels).mean()),
            "confusion_matrix": conf_matrix.tolist(),
            "per_class_iou": [],
            "per_class_dsc": [],
            "margin_stats": {
                "mean": float(margins.mean()),
                "std": float(margins.std()),
                "min": float(margins.min()),
                "max": float(margins.max()),
                "q25": float(np.percentile(margins, 25)),
                "q50": float(np.percentile(margins, 50)),
                "q75": float(np.percentile(margins, 75))
            }
        }
        
        # Per-class æŒ‡æ ‡
        for cls in range(1, self.num_classes):
            pred_mask = (train_preds == cls)
            label_mask = (train_labels == cls)
            tp = np.sum(pred_mask & label_mask)
            fp = np.sum(pred_mask & ~label_mask)
            fn = np.sum(~pred_mask & label_mask)
            
            if tp + fp + fn > 0:
                iou = tp / (tp + fp + fn)
                dsc_cls = 2 * tp / (2 * tp + fp + fn)
            else:
                iou = 0.0
                dsc_cls = 0.0
            
            metrics["per_class_iou"].append(float(iou))
            metrics["per_class_dsc"].append(float(dsc_cls))
        
        metrics_path = save_dir.parent / "_train_metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"   ðŸ’¾ _train_metrics.json (DSC={dsc:.4f}, Acc={metrics['accuracy']:.4f})")
        print(f"   ðŸ“Š Margin: mean={metrics['margin_stats']['mean']:.3f}, std={metrics['margin_stats']['std']:.3f}")
        print(f"   âœ… è®­ç»ƒè¯æ®ä¿å­˜å®Œæˆï¼")
    
    def _save_checkpoint_with_pipeline(self, ckpt_path: Path, sample_name: str, epoch: int, dsc: float):
        """
        ä¿å­˜åŒ…å«å®Œæ•´ pipeline å…ƒæ•°æ®å¥‘çº¦çš„ checkpoint
        
        è¿™ç¡®ä¿æŽ¨ç†æ—¶èƒ½å®Œå…¨å¤çŽ°è®­ç»ƒæ—¶çš„å‰å¤„ç†æµç¨‹
        """
        # æž„å»ºå®Œæ•´çš„ checkpoint
        single_dataset = getattr(self.dataloader, "dataset", None)
        train_ids_path_attr = getattr(single_dataset, "train_ids_path", None) if single_dataset is not None else None
        train_ids_path = Path(train_ids_path_attr) if train_ids_path_attr else Path("outputs/segmentation/overfit/_train_ids.npy")
        train_arrays_attr = getattr(single_dataset, "train_arrays_path", None) if single_dataset is not None else None
        train_arrays_path = Path(train_arrays_attr) if train_arrays_attr else None
        decim_hint = getattr(single_dataset, "decim_cache_vtp", None) if single_dataset is not None else None
        decim_cache_vtp = Path(decim_hint) if decim_hint else None
        if decim_cache_vtp is None:
            base_seg_dataset = getattr(single_dataset, "base_dataset", None) if single_dataset is not None else None
            if base_seg_dataset is not None and getattr(base_seg_dataset, "file_paths", None):
                sample_file = Path(base_seg_dataset.file_paths[0])
                target_cells = getattr(base_seg_dataset, "target_cells", None)
                if target_cells is not None:
                    decim_cache_vtp = DECIM_CACHE / f"{sample_file.stem}.c{int(target_cells)}.vtp"
        zscore_mean = self.mean.tolist() if self.mean is not None else None
        zscore_std = self.std.tolist() if self.std is not None else None
        knn_info = {
            "glm1": int(getattr(self.model, "k_short", 6)),
            "glm2": [int(getattr(self.model, "k_short", 6)), int(getattr(self.model, "k_long", 12))],
        }
        if decim_cache_vtp is None or not decim_cache_vtp.exists():
            raise FileNotFoundError(f"Decimated cache mesh not found for {sample_name}: {decim_cache_vtp}")
        if not train_ids_path.exists():
            raise FileNotFoundError(f"Training sample ids missing: {train_ids_path}")
        decim_cache_vtp_str = str(decim_cache_vtp.resolve())
        train_ids_path_str = str(train_ids_path.resolve())
        train_arrays_path_str = str(train_arrays_path.resolve()) if train_arrays_path and train_arrays_path.exists() else None
        fstn_module = getattr(self.model, "fstn", None)
        arch_config = {
            "glm_impl": getattr(self.model, "glm_impl", "edgeconv"),
            "use_feature_stn": bool(fstn_module),
            "k_short": int(getattr(self.model, "k_short", 6)),
            "k_long": int(getattr(self.model, "k_long", 12)),
            "with_dropout": bool(getattr(self.model, "with_dropout", False)),
            "dropout_p": float(getattr(self.model, "dropout_p", 0.0)),
        }

        feature_dim = int(getattr(self.model, "in_channels", 0) or getattr(single_dataset, "feature_dim", 18))

        checkpoint = {
            # æ¨¡åž‹æƒé‡
            "state_dict": self.model.state_dict(),
            
            # æ¨¡åž‹æž¶æž„ä¿¡æ¯
            "num_classes": self.num_classes,
            "in_channels": feature_dim,  # feature dimension propagated to inference
            "arch": arch_config,
            "train_sample_ids_path": train_ids_path_str,
            "train_arrays_path": train_arrays_path_str,

            # å‰å¤„ç† pipeline å¥‘çº¦
            "pipeline": {
                # Z-score æ ‡å‡†åŒ–
                "zscore": {
                    "mean": zscore_mean,
                    "std": zscore_std,
                    "apply": True
                },
                
                # å‡ ä½•é¢„å¤„ç†
                "centered": True,          # ç‰¹å¾æå–å‰å‡è´¨å¿ƒ
                "div_by_diag": True,        # ä½ç½®æŒ‰ç›’å¯¹è§’çº¿å½’ä¸€
                "use_frame": False,        # overfit ä¸ä½¿ç”¨ arch frame
                
                # é‡‡æ ·ç­–ç•¥
                "sampler": "class_balanced",  # overfit ä½¿ç”¨åˆ†å±‚é‡‡æ ·
                "sample_cells": 6000,      # é‡‡æ ·çš„ cell æ•°é‡
                "target_cells": 10000,     # æŠ½å–åŽçš„ç›®æ ‡ cell æ•°é‡
                
                # ç‰¹å¾å¸ƒå±€ï¼ˆç”¨äºŽæŽ¨ç†æ—¶çš„æ—‹è½¬å¯¹é½ï¼‰
                "feature_layout": {
                    "rotate_blocks": [
                        [0, 3],    # triangle vertex 0 relative to centroid
                        [3, 6],    # vertex 1
                        [6, 9],    # vertex 2
                        [9, 12],   # face normal
                        [12, 15]   # relative centroid
                    ],
                    "extra_blocks": [
                        [15, feature_dim]  # geometric cues (edge length, area, curvature)
                    ]
                },
                
                # éšæœºç§å­ï¼ˆä¾¿äºŽå¤çŽ°ï¼‰
                "seed": 42
            },
            
            # è®­ç»ƒä¿¡æ¯
            "training": {
                "sample_name": sample_name,
                "epoch": epoch,
                "best_dsc": dsc,
                "optimizer": "Adam",
                "lr": 0.01,
                "weight_decay": 0.0
            },
            
            # æ ‡ç­¾ä¿¡æ¯ï¼ˆFDI ç¼–ç  â†’ è¿žç»­ç´¢å¼•æ˜ å°„ï¼‰
            "label_mapping": {
                "fdi_labels": [11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27],
                "num_classes": self.num_classes,
                "background_class": 0
            }
        }
        
        checkpoint["pipeline"].update({
            "decim_cache_vtp": decim_cache_vtp_str,
            "train_ids_path": train_ids_path_str,
            "train_arrays_path": train_arrays_path_str,
            "diag_mode": "cells",
            "zscore_mean": zscore_mean,
            "zscore_std": zscore_std,
            "knn_k": {"to10k": 5, "tofull": 7}, 
            "train_sample_ids_path": train_ids_path_str,
            
        })
        
        checkpoint["training"].update({
            "seed": 42,
            "numpy_rng": 42,
            "torch_seed": 42,
        })
        torch.save(checkpoint, ckpt_path)
        print(f"ðŸ’¾ ä¿å­˜ checkpoint (å« pipeline å¥‘çº¦): {ckpt_path.name}")
    
    def train(self, epochs: int, save_dir: Path, sample_name: str):
        """æ‰§è¡Œè¿‡æ‹Ÿåˆè®­ç»ƒ"""
        print(f"\nðŸš€ å¼€å§‹å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒ:")
        print(f"   - æ ·æœ¬: {sample_name}")
        print(f"   - Epochs: {epochs}")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - å­¦ä¹ çŽ‡: {self.optimizer.param_groups[0]['lr']}")
        
        save_dir.mkdir(parents=True, exist_ok=True)
        best_dsc = 0.0
        
        for epoch in range(1, epochs + 1):
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # âš¡ ä¼˜åŒ–ï¼šåªåœ¨ç‰¹å®š epoch è¿›è¡Œè¯„ä¼°ï¼ˆå‡å°‘éªŒè¯å¼€é”€ï¼‰
            should_evaluate = (epoch == 1 or epoch % 10 == 0 or epoch == epochs)
            
            if should_evaluate:
                # è¯„ä¼°
                eval_metrics = self.evaluate()
                
                # è®°å½•åŽ†å²
                self.history['loss'].append(train_metrics['loss'])
                self.history['dice_loss'].append(train_metrics['dice_loss'])
                self.history['ce_loss'].append(train_metrics['ce_loss'])
                self.history['dsc'].append(eval_metrics['dsc'])
                self.history['accuracy'].append(train_metrics['accuracy'])
                self.history['train_bg0'].append(train_metrics['bg_ratio'])
                self.history['train_entropy'].append(train_metrics['entropy'])
                self.history['val_bg0'].append(eval_metrics['bg_ratio'])
                self.history['val_entropy'].append(eval_metrics['entropy'])
                self.history['bf1'].append(eval_metrics['bf1'])
                self.history['ger'].append(eval_metrics['ger'])
                self.history['ilr'].append(eval_metrics['ilr'])
                
                # ä¿å­˜æœ€ä½³æ¨¡åž‹ï¼ˆåŒ…å«å®Œæ•´çš„ pipeline å…ƒæ•°æ®å¥‘çº¦ï¼‰
                if eval_metrics['dsc'] > best_dsc:
                    best_dsc = eval_metrics['dsc']
                    self._save_checkpoint_with_pipeline(
                        save_dir / f"best_overfit_{sample_name}.pt",
                        sample_name,
                        epoch,
                        eval_metrics['dsc']
                    )
                
                # æ‰“å°è¿›åº¦ï¼ˆåŒ…å«æ–°è¯Šæ–­æŒ‡æ ‡ï¼‰
                print(f"Epoch {epoch:3d}/{epochs} | "
                      f"Loss: {train_metrics['loss']:.6f} | "
                      f"DSC: {eval_metrics['dsc']:.4f} | "
                      f"Acc: {train_metrics['accuracy']:.4f} | "
                      f"ðŸ” Train BG0: {train_metrics['bg_ratio']:.3f} | "
                      f"Train Ent: {train_metrics['entropy']:.3f} | "
                      f"Val BG0: {eval_metrics['bg_ratio']:.3f} | "
                      f"Val Ent: {eval_metrics['entropy']:.3f} | "
                      f"BF1@0.2: {eval_metrics['bf1']:.4f} | "
                      f"GER: {eval_metrics['ger']*100:.2f}% | "
                      f"ILR: {eval_metrics['ilr']*100:.2f}%", flush=True)
            else:
                # å¿«é€Ÿæ¨¡å¼ï¼šåªæ‰“å°è®­ç»ƒæŒ‡æ ‡ï¼Œä¸åšè¯„ä¼°
                if epoch % 5 == 0:  # æ¯ 5 ä¸ª epoch æ‰“å°ä¸€æ¬¡
                    print(f"Epoch {epoch:3d}/{epochs} | "
                          f"Loss: {train_metrics['loss']:.6f} | "
                          f"Acc: {train_metrics['accuracy']:.4f} | "
                          f"ðŸ” BG0: {train_metrics['bg_ratio']:.3f} | "
                          f"Ent: {train_metrics['entropy']:.3f}", flush=True)
        
        print(f"\nâœ… è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆ! æœ€ä½³DSC: {best_dsc:.4f}")
        
        # ðŸ”¬ å†³ç­–æ ‘èŠ‚ç‚¹1ï¼šä¿å­˜æœ€ç»ˆ epoch çš„ logits å’Œ labels
        print(f"\nðŸ”¬ ä¿å­˜è®­ç»ƒè¯æ®ï¼ˆå†³ç­–æ ‘èŠ‚ç‚¹1ï¼‰...")
        self._save_training_evidence(save_dir, sample_name)
        
        # ä¿å­˜è®­ç»ƒåŽ†å²
        self.save_training_plots(save_dir, sample_name)
        
        return best_dsc
    
    def save_training_plots(self, save_dir: Path, sample_name: str):
        """ä¿å­˜è®­ç»ƒæ›²çº¿å›¾"""
        epochs = range(1, len(self.history['loss']) + 1)
        
        plt.figure(figsize=(20, 12))
        
        # Loss curves
        plt.subplot(3, 3, 1)
        plt.plot(epochs, self.history['loss'], 'b-', label='Total Loss')
        plt.plot(epochs, self.history['dice_loss'], 'r-', label='Dice Loss')
        plt.plot(epochs, self.history['ce_loss'], 'g-', label='CE Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.legend()
        plt.yscale('log')
        
        # DSC curve
        plt.subplot(3, 3, 2)
        plt.plot(epochs, self.history['dsc'], 'b-', label='DSC')
        plt.xlabel('Epoch')
        plt.ylabel('DSC')
        plt.title('Dice Similarity Coefficient')
        plt.legend()
        
        # Accuracy curve
        plt.subplot(3, 3, 3)
        plt.plot(epochs, self.history['accuracy'], 'g-', label='Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('Training Accuracy')
        plt.legend()
        
        # BG0 ratio curves (key diagnostic)
        plt.subplot(3, 3, 4)
        plt.plot(epochs, self.history['train_bg0'], 'r-', label='Train BG0', linewidth=2)
        plt.plot(epochs, self.history['val_bg0'], 'b-', label='Val BG0', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('BG0 Ratio')
        plt.title('Background Prediction Ratio (should drop fast)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ðŸ” Entropy æ›²çº¿ï¼ˆå…³é”®è¯Šæ–­æŒ‡æ ‡ï¼‰
        plt.subplot(3, 3, 5)
        plt.plot(epochs, self.history['train_entropy'], 'r-', label='Train Entropy', linewidth=2)
        plt.plot(epochs, self.history['val_entropy'], 'b-', label='Val Entropy', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Entropy')
        plt.title('Prediction Entropy (should drop fast)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Boundary metrics
        ax6 = plt.subplot(3, 3, 6)
        if self.history['bf1']:
            ax6.plot(epochs, self.history['bf1'], 'b-', label='BF1@0.2')
            ax6.set_ylim(0.0, 1.0)
            ax6.set_ylabel('BF1')
            ax6.set_xlabel('Epoch')
            ax6.set_title('Boundary Metrics')
            ax6.grid(True, alpha=0.3)
            ax6_t = ax6.twinx()
            ger_percent = np.array(self.history['ger']) * 100.0
            ilr_percent = np.array(self.history['ilr']) * 100.0
            ax6_t.plot(epochs, ger_percent, 'r--', label='GER (%)')
            ax6_t.plot(epochs, ilr_percent, 'g-.', label='ILR (%)')
            ax6_t.set_ylabel('Percent (%)')
            lines, labels = ax6.get_legend_handles_labels()
            lines2, labels2 = ax6_t.get_legend_handles_labels()
            ax6_t.legend(lines + lines2, labels + labels2, loc='upper right')
        else:
            ax6.set_title('Boundary Metrics (N/A)')
        
        # Last 50 epochs loss
        if len(epochs) > 50:
            plt.subplot(3, 3, 7)
            last_50 = epochs[-50:]
            plt.plot(last_50, self.history['loss'][-50:], 'b-', label='Total Loss')
            plt.plot(last_50, self.history['dice_loss'][-50:], 'r-', label='Dice Loss')
            plt.plot(last_50, self.history['ce_loss'][-50:], 'g-', label='CE Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title('Last 50 Epochs - Loss')
            plt.legend()
            plt.yscale('log')
        
        # Last 50 epochs DSC
        if len(epochs) > 50:
            plt.subplot(3, 3, 8)
            plt.plot(last_50, self.history['dsc'][-50:], 'b-', label='DSC')
            plt.xlabel('Epoch')
            plt.ylabel('DSC')
            plt.title('Last 50 Epochs - DSC')
            plt.legend()
            
            # Last 50 epochs BG0 (key diagnostic)
            plt.subplot(3, 3, 9)
            plt.plot(last_50, self.history['train_bg0'][-50:], 'r-', label='Train BG0')
            plt.plot(last_50, self.history['val_bg0'][-50:], 'b-', label='Val BG0')
            plt.xlabel('Epoch')
            plt.ylabel('BG0 Ratio')
            plt.title('Last 50 Epochs - BG0 Ratio')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.suptitle(f'Single Sample Overfitting - {sample_name}', fontsize=16)
        plt.tight_layout()
        
        plot_file = save_dir / f"overfit_curves_{sample_name}.png"
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ðŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_file}")

def _save_overfit_checkpoint(model, ckpt_path: Path, *,
                             num_classes: int,
                             mean: np.ndarray, std: np.ndarray,
                             sample_cells: int = 6000,
                             target_cells: int = 10000,
                             train_ids_path: Path | None,
                             decim_cache_vtp: Path | None,
                             train_arrays_path: Path | None = None):
    fstn_module = getattr(model, "fstn", None)
    arch_config = {
        "glm_impl": getattr(model, "glm_impl", "edgeconv"),
        "use_feature_stn": bool(fstn_module),
        "k_short": int(getattr(model, "k_short", 6)),
        "k_long": int(getattr(model, "k_long", 12)),
        "with_dropout": bool(getattr(model, "with_dropout", False)),
        "dropout_p": float(getattr(model, "dropout_p", 0.0)),
    }
    train_ids_str = str(train_ids_path.resolve()) if train_ids_path else None
    train_arrays_str = str(train_arrays_path.resolve()) if train_arrays_path else None
    decim_cache_str = str(decim_cache_vtp.resolve()) if decim_cache_vtp else None

    payload = {
        "state_dict": model.state_dict(),
        "num_classes": int(num_classes),
        "in_channels": feature_dim,
        "arch": arch_config,
        # å…¼å®¹å­—æ®µï¼ˆè€ç‰ˆæœ¬ä¼šä»Žè¿™äº›é”®è¯»å–ï¼‰
        "train_sample_ids_path": train_ids_str,
        "train_arrays_path": train_arrays_str,
        # ç»Ÿä¸€å¥‘çº¦ï¼ˆæŽ¨ç†ç«¯ä¼˜å…ˆä½¿ç”¨è¿™é‡Œçš„å­—æ®µï¼‰
        "pipeline": {
            "zscore": {
                "apply": True,
                "mean": mean.astype(np.float32).tolist(),
                "std":  np.clip(std, 1e-6, None).astype(np.float32).tolist(),
            },
            "centered": True,          # è®­ç»ƒæ—¶ mesh.points -= center
            "div_by_diag": True,       # è®­ç»ƒä½ç½® pos_norm / diag
            "use_frame": False,        # è‹¥åŽç»­æä¾› arch frame å¯åˆ‡ True
            "sampler": "random",       # è®­ç»ƒ 6k éšæœºé‡‡æ ·ï¼ˆæŽ¨ç†å¤ç”¨ train_idsï¼‰
            "sample_cells": int(sample_cells),
            "target_cells": int(target_cells),
            "train_ids_path": train_ids_str,
            "train_arrays_path": train_arrays_str,
            "decim_cache_vtp": decim_cache_str,
            # è®°å½• knn kï¼ˆä¾›åŽå¤„ç†å…œåº•ï¼‰
            "knn_k": {"to10k": 5, "tofull": 7},
            "diag_mode": "cells",
            "seed": 42,
        },
    }
    ckpt_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(payload, str(ckpt_path))
    print(f"ðŸ’¾ ä¿å­˜ checkpoint (å« pipeline å¥‘çº¦): {ckpt_path.name}")
    
def main():
    parser = argparse.ArgumentParser(description="å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒ")
    parser.add_argument("--sample", type=str, required=True, 
                       help="æ ·æœ¬åç§°ï¼Œå¦‚ 1_L æˆ– 5_U")
    parser.add_argument("--epochs", type=int, default=300,
                       help="è®­ç»ƒepochæ•° (é»˜è®¤: 300)")
    parser.add_argument("--dataset-root", type=str, default="datasets/segmentation_dataset",
                       help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ (cuda/cpu/auto)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ðŸŽ¯ iMeshSegNet å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒ")
    print("=" * 60)
    print(f"æ ·æœ¬: {args.sample}")
    print(f"Epochs: {args.epochs}")
    print(f"æ•°æ®é›†: {args.dataset_root}")
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    print(f"è®¾å¤‡: {device}")
    
    # è®¾ç½®æ•°æ®
    dataset_root = Path(args.dataset_root)
    dataloader, num_classes, mean, std = setup_single_sample_training(args.sample, dataset_root)
    
    # åˆ›å»ºæ¨¡åž‹
    print(f"\nðŸ—ï¸  åˆ›å»ºæ¨¡åž‹ (ç±»åˆ«æ•°: {num_classes})")
    model = iMeshSegNet(num_classes=num_classes, with_dropout=False, use_feature_stn=False)
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ å…¥ mean, std ç”¨äºŽä¿å­˜ pipeline å¥‘çº¦ï¼‰
    trainer = OverfitTrainer(model, dataloader, num_classes, device, mean, std)
    
    # è¾“å‡ºç›®å½•
    output_dir = Path("outputs/segmentation/overfit") / args.sample
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    best_dsc = trainer.train(args.epochs, output_dir, args.sample)
    end_time = time.time()
    
    print(f"\nðŸŽ‰ è®­ç»ƒå®Œæˆ!")
    print(f"   - æœ€ä½³DSC: {best_dsc:.4f}")
    print(f"   - è®­ç»ƒæ—¶é—´: {end_time - start_time:.1f}ç§’")
    print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    # æœŸæœ›ç»“æžœåˆ†æž
    print(f"\nðŸ“ˆ ç»“æžœåˆ†æž:")
    if best_dsc > 0.95:
        print(f"   âœ… ä¼˜ç§€! DSC > 0.95ï¼Œæ¨¡åž‹èƒ½å¤Ÿå®Œç¾Žè¿‡æ‹Ÿåˆå•æ ·æœ¬")
    elif best_dsc > 0.8:
        print(f"   âœ… è‰¯å¥½! DSC > 0.8ï¼Œæ¨¡åž‹åŸºæœ¬æ­£å¸¸ï¼Œå¯èƒ½éœ€è¦æ›´å¤šepoch")
    elif best_dsc > 0.5:
        print(f"   âš ï¸  ä¸€èˆ¬! DSC > 0.5ï¼Œæ¨¡åž‹å­¦ä¹ èƒ½åŠ›æœ‰é™ï¼Œæ£€æŸ¥æ•°æ®æˆ–æ¨¡åž‹ç»“æž„")
    else:
        print(f"   âŒ å¼‚å¸¸! DSC < 0.5ï¼Œå­˜åœ¨ä¸¥é‡é—®é¢˜:")
        print(f"      - æ£€æŸ¥æ•°æ®æ ¼å¼å’Œæ ‡ç­¾æ˜ å°„")
        print(f"      - æ£€æŸ¥æ¨¡åž‹è¾“å…¥è¾“å‡ºç»´åº¦")
        print(f"      - æ£€æŸ¥æŸå¤±å‡½æ•°é…ç½®")


if __name__ == "__main__":
    main()


