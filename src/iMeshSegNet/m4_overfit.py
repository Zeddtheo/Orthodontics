#!/usr/bin/env python3
"""
m5_overfit.py - å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒè„šæœ¬
ç”¨äºéªŒè¯æ¨¡å‹å’Œè®­ç»ƒç®¡é“çš„æ­£ç¡®æ€§ï¼Œé€šè¿‡åœ¨å•ä¸ªæ ·æœ¬ä¸Šè¿‡æ‹Ÿåˆæ¥å¿«é€Ÿè¯Šæ–­é—®é¢˜ã€‚

ä½¿ç”¨æ–¹æ³•:
    python m5_overfit.py --sample 1_L  # å¯¹1_L.vtpè¿›è¡Œè¿‡æ‹Ÿåˆ
    python m5_overfit.py --sample 5_U --epochs 500  # è‡ªå®šä¹‰epochæ•°
"""

import argparse
import sys
from pathlib import Path
from typing import List, Tuple
import time

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.amp import autocast, GradScaler
from torch.nn.utils import clip_grad_norm_
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import pyvista as pv

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent.parent))

from m0_dataset import (
    DECIM_CACHE,
    FDI_LABELS,
    LABEL_REMAP,
    SEG_NUM_CLASSES,
    SINGLE_ARCH_NUM_CLASSES,
    _build_single_arch_label_maps,
    _load_or_build_decimated_mm,
    extract_features,
    find_label_array,
    normalize_mesh_units,
    remap_labels_single_arch,
    remap_segmentation_labels,
)
from m1_train import GeneralizedDiceLoss
from imeshsegnet import iMeshSegNet


def calculate_metrics(preds: torch.Tensor, labels: torch.Tensor, num_classes: int) -> Tuple[float, float, float]:
    """
    è®¡ç®— DSC, Sensitivity, PPV
    
    Args:
        preds: é¢„æµ‹æ ‡ç­¾ (N,)
        labels: çœŸå®æ ‡ç­¾ (N,)
        num_classes: ç±»åˆ«æ•°
        
    Returns:
        (dsc, sensitivity, ppv)
    """
    dsc_list = []
    sen_list = []
    ppv_list = []
    
    for cls in range(1, num_classes):  # è·³è¿‡èƒŒæ™¯ç±»
        pred_mask = (preds == cls)
        label_mask = (labels == cls)
        
        tp = (pred_mask & label_mask).sum().item()
        fp = (pred_mask & ~label_mask).sum().item()
        fn = (~pred_mask & label_mask).sum().item()
        
        if tp + fp + fn == 0:
            continue
            
        dsc = 2 * tp / (2 * tp + fp + fn + 1e-8)
        sen = tp / (tp + fn + 1e-8)
        ppv_val = tp / (tp + fp + 1e-8)
        
        dsc_list.append(dsc)
        sen_list.append(sen)
        ppv_list.append(ppv_val)
    
    return (
        float(np.mean(dsc_list)) if dsc_list else 0.0,
        float(np.mean(sen_list)) if sen_list else 0.0,
        float(np.mean(ppv_list)) if ppv_list else 0.0
    )


DEFAULT_LABEL_MODE = "single_arch_16"
DEFAULT_GINGIVA_SRC = 0
DEFAULT_GINGIVA_CLASS_ID = 15
DEFAULT_KEEP_VOID_ZERO = False
FDI_LABEL_SET = set(FDI_LABELS)
INV_LABEL_REMAP = {v: k for k, v in LABEL_REMAP.items()}


class SingleSampleDataset(Dataset):
    """å•æ ·æœ¬æ•°æ®é›† - ç”Ÿæˆä¸€æ¬¡æ ·æœ¬ååå¤è¿”å›"""

    def __init__(
        self,
        sample_file: str,
        mean: np.ndarray,
        std: np.ndarray,
        *,
        label_mode: str = DEFAULT_LABEL_MODE,
        gingiva_src_label: int = DEFAULT_GINGIVA_SRC,
        gingiva_class_id: int = DEFAULT_GINGIVA_CLASS_ID,
        keep_void_zero: bool = DEFAULT_KEEP_VOID_ZERO,
    ):
        target_cells = 10000
        sample_cells = 6000
        sample_path = Path(sample_file)

        decimated_mesh = _load_or_build_decimated_mm(sample_path, target_cells=target_cells)
        self.decim_cache_vtp = (DECIM_CACHE / f"{sample_path.stem}.c{target_cells}.vtp").resolve()

        mesh = decimated_mesh.copy(deep=True)
        mesh.points -= mesh.center

        dec_label_info = find_label_array(mesh)
        if dec_label_info is None:
            raise RuntimeError(f"{self.decim_cache_vtp} ç¼ºå¤± Label æ•°ç»„")
        _, dec_labels = dec_label_info
        dec_labels = np.asarray(dec_labels, dtype=np.int64)

        full_mesh = pv.read(str(sample_path))
        full_label_info = find_label_array(full_mesh)
        if full_label_info is None:
            raise RuntimeError(f"{sample_file} ç¼ºå¤± Label æ•°ç»„")
        _, full_raw_labels = full_label_info
        full_raw_labels = np.asarray(full_raw_labels, dtype=np.int64, copy=False)

        orig_ids = mesh.cell_data.get("vtkOriginalCellIds")
        if orig_ids is not None and full_raw_labels.size > 0:
            orig_ids_np = np.asarray(orig_ids, dtype=np.int64, copy=False)
            base_labels = full_raw_labels[orig_ids_np]
        else:
            base_labels = dec_labels

        self.label_mode = label_mode
        self._label_kwargs = {
            "gingiva_src_label": gingiva_src_label,
            "gingiva_class_id": gingiva_class_id,
            "keep_void_zero": keep_void_zero,
        }
        debug_info = {
            "raw_unique": np.unique(base_labels).astype(int).tolist(),
            "intermediate_unique": None,
            "fallback_values": [],
        }
        if label_mode == "single_arch_16":
            raw_unique = np.unique(base_labels)
            needs_inverse = any(
                (val not in FDI_LABEL_SET) and (val != gingiva_src_label) and (val != 0)
                for val in raw_unique
            )
            if needs_inverse:
                base_labels = np.asarray(
                    [INV_LABEL_REMAP.get(int(v), 0) for v in base_labels],
                    dtype=np.int64,
                )
            debug_info["intermediate_unique"] = np.unique(base_labels).astype(int).tolist()
        if label_mode == "single_arch_16":
            single_arch_maps = _build_single_arch_label_maps(
                gingiva_src_label,
                gingiva_class_id,
                keep_void_zero,
            )
            labels = remap_labels_single_arch(base_labels, sample_path, single_arch_maps)
            self.num_classes_full = SINGLE_ARCH_NUM_CLASSES
            debug_info["post_unique"] = np.unique(labels).astype(int).tolist()
        elif label_mode == "full_fdi":
            labels = remap_segmentation_labels(base_labels)
            self.num_classes_full = SEG_NUM_CLASSES
            debug_info["post_unique"] = np.unique(labels).astype(int).tolist()
        else:
            raise ValueError(f"Unsupported label_mode: {label_mode}")

        mesh, _, _, diag_after = normalize_mesh_units(mesh)
        mesh = mesh.triangulate()

        labels_full_unique = np.unique(labels).astype(np.int64, copy=False)
        self.labels_full_unique = labels_full_unique

        feats = extract_features(mesh).astype(np.float32, copy=False)
        pos_mm = mesh.cell_centers().points.astype(np.float32)
        pos_scale = diag_after if diag_after > 1e-6 else 1.0
        pos_norm = pos_mm / pos_scale

        rng = np.random.default_rng(42)
        if feats.shape[0] > sample_cells:
            total_cells = feats.shape[0]
            min_samples_per_class = 160
            required_indices: List[int] = []
            unique_labels = np.unique(labels)
            for cls in unique_labels:
                if cls == 0:
                    continue
                cls_indices = np.where(labels == cls)[0]
                if cls_indices.size == 0:
                    continue
                quota = min(min_samples_per_class, sample_cells)
                take = min(cls_indices.size, quota)
                chosen = rng.choice(cls_indices, size=take, replace=cls_indices.size < take)
                required_indices.extend(chosen.tolist())

            if required_indices:
                required_array = np.unique(np.asarray(required_indices, dtype=np.int64))
            else:
                required_array = np.empty(0, dtype=np.int64)

            slots_left = sample_cells - required_array.size
            if slots_left < 0:
                idx = rng.choice(required_array, size=sample_cells, replace=False)
            else:
                mask = np.ones(total_cells, dtype=bool)
                if required_array.size > 0:
                    mask[required_array] = False
                pool = np.nonzero(mask)[0]
                extra_take = min(slots_left, pool.size)
                extra = rng.choice(pool, size=extra_take, replace=False) if extra_take > 0 else np.empty(0, dtype=np.int64)
                if required_array.size > 0:
                    idx = np.concatenate([required_array, extra])
                else:
                    idx = extra
                if idx.size < sample_cells:
                    deficit = sample_cells - idx.size
                    fallback = rng.choice(total_cells, size=deficit, replace=True)
                    idx = np.concatenate([idx, fallback])
            idx = idx[:sample_cells]
            rng.shuffle(idx)
            feats = feats[idx]
            pos_mm = pos_mm[idx]
            pos_norm = pos_norm[idx]
            labels = labels[idx]
        else:
            idx = np.arange(feats.shape[0], dtype=np.int64)

        self.sample_indices = idx.astype(np.int64, copy=False)

        feats_norm = (feats - mean.reshape(1, -1)) / np.clip(std.reshape(1, -1), 1e-6, None)
        features_tensor = torch.from_numpy(feats_norm.astype(np.float32)).transpose(0, 1).contiguous()
        pos_tensor = torch.from_numpy(pos_norm.astype(np.float32)).transpose(0, 1).contiguous()
        pos_mm_tensor = torch.from_numpy(pos_mm.astype(np.float32)).transpose(0, 1).contiguous()
        pos_scale_tensor = torch.tensor([pos_scale], dtype=torch.float32)
        labels_tensor = torch.from_numpy(labels.astype(np.int64))

        self.sample_data = ((features_tensor, pos_tensor, pos_mm_tensor, pos_scale_tensor), labels_tensor)
        self.remap_debug = debug_info

    def __len__(self) -> int:
        return 10

    def __getitem__(self, idx):
        return self.sample_data


def setup_single_sample_training(sample_name: str, dataset_root: Path) -> Tuple[DataLoader, int, np.ndarray, np.ndarray]:
    """
    è®¾ç½®å•æ ·æœ¬è®­ç»ƒæ•°æ®
    
    Args:
        sample_name: æ ·æœ¬åç§°ï¼Œå¦‚ "1_L" æˆ– "5_U"
        dataset_root: æ•°æ®é›†æ ¹ç›®å½•
        
    Returns:
        (dataloader, num_classes, mean, std): æ•°æ®åŠ è½½å™¨ã€ç±»åˆ«æ•°å’Œæ ‡å‡†åŒ–å‚æ•°
    """
    print(f"ğŸ” è®¾ç½®å•æ ·æœ¬è®­ç»ƒ: {sample_name}")
    
    # æ„å»ºæ ·æœ¬æ–‡ä»¶è·¯å¾„
    sample_file = dataset_root / f"{sample_name}.vtp"
    if not sample_file.exists():
        raise FileNotFoundError(f"æ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {sample_file}")
    
    # åŠ è½½ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¼˜å…ˆå¤ç”¨ç¼“å­˜ï¼Œç¼ºå¤±åˆ™å°±åœ°è®¡ç®—ï¼‰
    stats_path = Path("outputs/segmentation/overfit/_stats_sample.npz")
    if stats_path.exists():
        with np.load(stats_path) as stats:
            mean = stats["mean"].astype(np.float32, copy=False)
            std = stats["std"].astype(np.float32, copy=False)
        std = np.clip(std, 1e-6, None)
        print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»Ÿè®¡: {stats_path}")
    else:
        mesh_mm = _load_or_build_decimated_mm(sample_file, target_cells=10000)
        mesh = mesh_mm.copy(deep=True)
        mesh.points -= mesh.center
        mesh, *_ = normalize_mesh_units(mesh)
        mesh = mesh.triangulate()
        feats = extract_features(mesh).astype(np.float32, copy=False)
        mean = feats.mean(axis=0).astype(np.float32, copy=False)
        std = np.clip(feats.std(axis=0), 1e-6, None).astype(np.float32, copy=False)
        stats_path.parent.mkdir(parents=True, exist_ok=True)
        np.savez(stats_path, mean=mean, std=std)
        print(f"[overfit] built sample stats -> {stats_path}")

    # åˆ›å»ºå•æ ·æœ¬æ•°æ®é›†
    single_dataset = SingleSampleDataset(
        str(sample_file),
        mean,
        std,
        label_mode=DEFAULT_LABEL_MODE,
        gingiva_src_label=DEFAULT_GINGIVA_SRC,
        gingiva_class_id=DEFAULT_GINGIVA_CLASS_ID,
        keep_void_zero=DEFAULT_KEEP_VOID_ZERO,
    )
    remap_debug = getattr(single_dataset, "remap_debug", None)

    # è·å–ä¸€ä¸ªæ ·æœ¬æ¥ç¡®å®šç±»åˆ«æ•°
    sample_data = single_dataset[0]
    # æ•°æ®æ ¼å¼: ((features, pos, pos_mm, pos_scale), labels)
    (features, pos, pos_mm, pos_scale), labels = sample_data
    
    # ğŸ’¾ ä¿å­˜è®­ç»ƒä¾§æ•°ç»„ï¼ˆç”¨äºä¸æ¨ç†å¯¹æ¯”ï¼‰
    # æ³¨æ„ï¼šfeatures å’Œ pos æ ¼å¼æ˜¯ (C, N)ï¼Œéœ€è¦è½¬ç½®ä¸º (N, C) ä»¥ä¾¿å¯¹æ¯”
    train_arrays_path = Path("outputs/segmentation/overfit/_train_arrays.npz")
    train_arrays_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(str(train_arrays_path),
             feats=features.transpose(0, 1).numpy(),  # (15, N) -> (N, 15)
             pos=pos.transpose(0, 1).numpy())          # (3, N) -> (N, 3)
    print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒä¾§æ•°ç»„: {train_arrays_path} (shape: feats={features.shape}->{features.transpose(0,1).shape}, pos={pos.shape}->{pos.transpose(0,1).shape})")
    single_dataset.train_arrays_path = train_arrays_path.resolve()
    
    # ğŸ”¬ ä¿å­˜è®­ç»ƒæ—¶çš„é‡‡æ ·ç´¢å¼•ï¼ˆç”¨äºæ¨ç†ç«¯å¤ç”¨ï¼‰
    if hasattr(single_dataset, "sample_indices"):
        train_ids_path = Path("outputs/segmentation/overfit/_train_ids.npy")
        np.save(str(train_ids_path), np.asarray(single_dataset.sample_indices, dtype=np.int64))
        print(f"ğŸ’¾ ä¿å­˜è®­ç»ƒé‡‡æ ·ç´¢å¼•: {train_ids_path} (shape: {np.asarray(single_dataset.sample_indices).shape})")
        single_dataset.train_ids_path = train_ids_path.resolve()
    
    unique_labels = torch.unique(labels)
    default_classes = SINGLE_ARCH_NUM_CLASSES if single_dataset.label_mode == "single_arch_16" else SEG_NUM_CLASSES
    num_classes = int(getattr(single_dataset, "num_classes_full", default_classes))
    labels_full_unique = getattr(single_dataset, "labels_full_unique", None)

    # åˆ›å»ºDataLoader
    # âš¡ ä¼˜åŒ–ï¼šbatch_size=2 é¿å… BatchNorm batch_size=1 é—®é¢˜ï¼ŒåŒæ—¶åŠ é€Ÿè®­ç»ƒ
    dataloader = DataLoader(
        single_dataset,
        batch_size=2,
        shuffle=False,  # å•æ ·æœ¬ä¸éœ€è¦shuffle
        num_workers=0,
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"âœ… å•æ ·æœ¬æ•°æ®é›†è®¾ç½®å®Œæˆ")
    print(f"   - æ ·æœ¬å½¢çŠ¶: features={features.shape}, pos={pos.shape}, labels={labels.shape}")
    print(f"   - æ ‡ç­¾æ¨¡å¼: {single_dataset.label_mode}")
    if remap_debug is not None:
        print(f"   - åŸå§‹æ ‡ç­¾ unique: {remap_debug.get('raw_unique')}")
        if remap_debug.get('intermediate_unique') is not None:
            print(f"   - è½¬æ¢åæ ‡ç­¾ unique: {remap_debug.get('intermediate_unique')}")
    if labels_full_unique is not None:
        print(f"   - å…¨10kå”¯ä¸€æ ‡ç­¾: {labels_full_unique.tolist()}")
    print(f"   - 6kå”¯ä¸€æ ‡ç­¾: {unique_labels.tolist()}")
    print(f"   - ç±»åˆ«æ•°: {num_classes}")
    
    return dataloader, num_classes, mean, std


class OverfitTrainer:
    """å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, dataloader: DataLoader, num_classes: int, device: torch.device, 
                 mean: np.ndarray = None, std: np.ndarray = None):
        self.model = model.to(device)
        self.dataloader = dataloader
        self.num_classes = num_classes
        self.device = device
        self.mean = mean
        self.std = std
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        self.dice_loss = GeneralizedDiceLoss()
        self.ce_loss = nn.CrossEntropyLoss()
        
        # è®¾ç½®ä¼˜åŒ–å™¨ - ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡è¿›è¡Œå¿«é€Ÿè¿‡æ‹Ÿåˆ
        self.optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=0.01,  # æ¯”æ­£å¸¸è®­ç»ƒå¤§10å€
            weight_decay=0.0  # ç§»é™¤æƒé‡è¡°å‡ä»¥ä¾¿è¿‡æ‹Ÿåˆ
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        device_type = 'cuda' if device.type == 'cuda' else 'cpu'
        self.scaler = GradScaler(device_type) if device.type == 'cuda' else None
        self.use_amp = device.type == 'cuda'
        self.device_type = device_type  # ä¿å­˜ç”¨äº autocast
        
        # è®°å½•è®­ç»ƒå†å²
        self.history = {
            'loss': [],
            'dice_loss': [],
            'ce_loss': [],
            'dsc': [],
            'accuracy': [],
            'train_bg0': [],  # è®­ç»ƒé›†èƒŒæ™¯æ¯”ä¾‹
            'train_entropy': [],  # è®­ç»ƒé›†é¢„æµ‹ç†µ
            'val_bg0': [],  # éªŒè¯é›†èƒŒæ™¯æ¯”ä¾‹
            'val_entropy': []  # éªŒè¯é›†é¢„æµ‹ç†µ
        }
        
    def train_epoch(self) -> dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        # âš¡ batch_size=2ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç† BatchNorm
        self.model.train()
        epoch_metrics = {
            'loss': 0.0,
            'dice_loss': 0.0,
            'ce_loss': 0.0,
            'correct': 0,
            'total': 0
        }
        
        for batch_idx, batch_data in enumerate(self.dataloader):
            # è§£æbatchæ•°æ®: ((features, pos, pos_mm, pos_scale), labels)
            (features, pos, pos_mm, pos_scale), labels = batch_data
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            features = features.to(self.device, non_blocking=True)
            pos = pos.to(self.device, non_blocking=True)
            labels = labels.to(self.device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad(set_to_none=True)
            
            if self.use_amp:
                with autocast(self.device_type):
                    logits = self.model(features, pos)
                    dice_loss = self.dice_loss(logits, labels)
                    ce_loss = self.ce_loss(logits, labels)
                    total_loss = dice_loss + ce_loss
                
                assert self.scaler is not None
                self.scaler.scale(total_loss).backward()
                self.scaler.unscale_(self.optimizer)
                clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                logits = self.model(features, pos)
                dice_loss = self.dice_loss(logits, labels)
                ce_loss = self.ce_loss(logits, labels)
                total_loss = dice_loss + ce_loss
                
                total_loss.backward()
                clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            preds = torch.argmax(logits, dim=1)
            correct = (preds == labels).sum().item()
            total = labels.numel()
            
            # ğŸ” æ–°å¢è¯Šæ–­æŒ‡æ ‡ï¼šBG0 å’Œ Entropy
            with torch.no_grad():
                probs = torch.softmax(logits.detach(), dim=1)  # (B, C, N)
                # BG0: èƒŒæ™¯ç±»ï¼ˆç±»åˆ«0ï¼‰çš„é¢„æµ‹æ¯”ä¾‹
                bg_ratio = (preds == 0).float().mean().item()
                # Entropy: é¢„æµ‹ç†µï¼ˆè¡¡é‡ä¸ç¡®å®šæ€§ï¼‰
                entropy = -(probs * torch.log(probs.clamp(min=1e-8))).sum(dim=1).mean().item()
            
            if 'bg_ratio' not in epoch_metrics:
                epoch_metrics['bg_ratio'] = 0.0
                epoch_metrics['entropy'] = 0.0
            
            # ç´¯ç§¯æŒ‡æ ‡
            epoch_metrics['loss'] += total_loss.item()
            epoch_metrics['dice_loss'] += dice_loss.item()
            epoch_metrics['ce_loss'] += ce_loss.item()
            epoch_metrics['correct'] += correct
            epoch_metrics['total'] += total
            epoch_metrics['bg_ratio'] += bg_ratio
            epoch_metrics['entropy'] += entropy
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        num_batches = len(self.dataloader)
        epoch_metrics['loss'] /= num_batches
        epoch_metrics['dice_loss'] /= num_batches
        epoch_metrics['ce_loss'] /= num_batches
        epoch_metrics['accuracy'] = epoch_metrics['correct'] / epoch_metrics['total']
        epoch_metrics['bg_ratio'] /= num_batches
        epoch_metrics['entropy'] /= num_batches
        
        return epoch_metrics
    
    def evaluate(self) -> dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        all_preds = []
        all_labels = []
        all_bg_ratios = []
        all_entropy = []
        
        with torch.no_grad():
            for batch_data in self.dataloader:
                # è§£æbatchæ•°æ®: ((features, pos, pos_mm, pos_scale), labels)
                (features, pos, pos_mm, pos_scale), labels = batch_data
                
                features = features.to(self.device, non_blocking=True)
                pos = pos.to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                if self.use_amp:
                    with autocast(self.device_type):
                        logits = self.model(features, pos)
                else:
                    logits = self.model(features, pos)
                
                preds = torch.argmax(logits, dim=1)
                probs = torch.softmax(logits, dim=1)
                
                # è®¡ç®—è¯Šæ–­æŒ‡æ ‡
                bg_ratio = (preds == 0).float().mean().item()
                entropy = -(probs * torch.log(probs.clamp(min=1e-8))).sum(dim=1).mean().item()
                
                all_preds.append(preds.cpu())
                all_labels.append(labels.cpu())
                all_bg_ratios.append(bg_ratio)
                all_entropy.append(entropy)
        
        # è®¡ç®—DSC
        preds_tensor = torch.cat(all_preds)
        labels_tensor = torch.cat(all_labels)
        dsc, sen, ppv = calculate_metrics(preds_tensor, labels_tensor, self.num_classes)
        
        return {
            'dsc': dsc, 
            'sensitivity': sen, 
            'ppv': ppv,
            'bg_ratio': np.mean(all_bg_ratios),
            'entropy': np.mean(all_entropy)
        }
    
    def _save_training_evidence(self, save_dir: Path, sample_name: str):
        """
        ğŸ”¬ å†³ç­–æ ‘èŠ‚ç‚¹1ï¼šä¿å­˜è®­ç»ƒè¯æ®ï¼ˆlogits, labels, metricsï¼‰
        
        ç”¨äº --replay-train æ¨¡å¼ä¸‹éªŒè¯æ¨ç†æ˜¯å¦å®Œå…¨å¤ç°è®­ç»ƒå‰å‘
        """
        import json
        
        self.model.eval()
        all_logits = []
        all_labels = []
        all_preds = []
        
        print("   ğŸ“Š æ”¶é›†æœ€ç»ˆ epoch çš„ logits å’Œ labels...")
        
        with torch.no_grad():
            for batch_data in self.dataloader:
                (features, pos, pos_mm, pos_scale), labels = batch_data
                features = features.to(self.device, non_blocking=True)
                pos = pos.to(self.device, non_blocking=True)
                labels = labels.to(self.device, non_blocking=True)
                
                logits = self.model(features, pos)  # (B, C, N)
                preds = torch.argmax(logits, dim=1)  # (B, N)
                
                all_logits.append(logits.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
                all_preds.append(preds.cpu().numpy())
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        all_logits = np.concatenate(all_logits, axis=0)  # (B, C, N)
        all_labels = np.concatenate(all_labels, axis=0)  # (B, N)
        all_preds = np.concatenate(all_preds, axis=0)    # (B, N)
        
        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆå•æ ·æœ¬è®­ç»ƒï¼‰
        train_logits = all_logits[0]  # (C, N)
        train_labels = all_labels[0]  # (N,)
        train_preds = all_preds[0]    # (N,)
        
        # ä¿å­˜ logits å’Œ labels
        logits_path = save_dir.parent / "_train_logits.npy"
        labels_path = save_dir.parent / "_train_labels.npy"
        np.save(str(logits_path), train_logits)
        np.save(str(labels_path), train_labels)
        print(f"   ğŸ’¾ _train_logits.npy (shape: {train_logits.shape})")
        print(f"   ğŸ’¾ _train_labels.npy (shape: {train_labels.shape})")
        
        # è®¡ç®—å¹¶ä¿å­˜ metrics
        dsc, sen, ppv = calculate_metrics(
            torch.from_numpy(train_preds),
            torch.from_numpy(train_labels),
            self.num_classes
        )
        
        # è®¡ç®—æ··æ·†çŸ©é˜µ
        conf_matrix = np.zeros((self.num_classes, self.num_classes), dtype=np.int64)
        for true_label in range(self.num_classes):
            for pred_label in range(self.num_classes):
                conf_matrix[true_label, pred_label] = np.sum(
                    (train_labels == true_label) & (train_preds == pred_label)
                )
        
        # è®¡ç®— margin (ç½®ä¿¡åº¦)
        probs = np.exp(train_logits) / np.exp(train_logits).sum(axis=0, keepdims=True)  # Softmax
        sorted_probs = np.sort(probs, axis=0)
        margins = sorted_probs[-1, :] - sorted_probs[-2, :]  # p_max - p_second
        
        metrics = {
            "sample_name": sample_name,
            "num_classes": int(self.num_classes),
            "num_cells": int(train_labels.shape[0]),
            "dsc": float(dsc),
            "sensitivity": float(sen),
            "ppv": float(ppv),
            "accuracy": float((train_preds == train_labels).mean()),
            "confusion_matrix": conf_matrix.tolist(),
            "per_class_iou": [],
            "per_class_dsc": [],
            "margin_stats": {
                "mean": float(margins.mean()),
                "std": float(margins.std()),
                "min": float(margins.min()),
                "max": float(margins.max()),
                "q25": float(np.percentile(margins, 25)),
                "q50": float(np.percentile(margins, 50)),
                "q75": float(np.percentile(margins, 75))
            }
        }
        
        # Per-class æŒ‡æ ‡
        for cls in range(1, self.num_classes):
            pred_mask = (train_preds == cls)
            label_mask = (train_labels == cls)
            tp = np.sum(pred_mask & label_mask)
            fp = np.sum(pred_mask & ~label_mask)
            fn = np.sum(~pred_mask & label_mask)
            
            if tp + fp + fn > 0:
                iou = tp / (tp + fp + fn)
                dsc_cls = 2 * tp / (2 * tp + fp + fn)
            else:
                iou = 0.0
                dsc_cls = 0.0
            
            metrics["per_class_iou"].append(float(iou))
            metrics["per_class_dsc"].append(float(dsc_cls))
        
        metrics_path = save_dir.parent / "_train_metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"   ğŸ’¾ _train_metrics.json (DSC={dsc:.4f}, Acc={metrics['accuracy']:.4f})")
        print(f"   ğŸ“Š Margin: mean={metrics['margin_stats']['mean']:.3f}, std={metrics['margin_stats']['std']:.3f}")
        print(f"   âœ… è®­ç»ƒè¯æ®ä¿å­˜å®Œæˆï¼")
    
    def _save_checkpoint_with_pipeline(self, ckpt_path: Path, sample_name: str, epoch: int, dsc: float):
        """
        ä¿å­˜åŒ…å«å®Œæ•´ pipeline å…ƒæ•°æ®å¥‘çº¦çš„ checkpoint
        
        è¿™ç¡®ä¿æ¨ç†æ—¶èƒ½å®Œå…¨å¤ç°è®­ç»ƒæ—¶çš„å‰å¤„ç†æµç¨‹
        """
        # æ„å»ºå®Œæ•´çš„ checkpoint
        single_dataset = getattr(self.dataloader, "dataset", None)
        train_ids_path_attr = getattr(single_dataset, "train_ids_path", None) if single_dataset is not None else None
        train_ids_path = Path(train_ids_path_attr) if train_ids_path_attr else Path("outputs/segmentation/overfit/_train_ids.npy")
        train_arrays_attr = getattr(single_dataset, "train_arrays_path", None) if single_dataset is not None else None
        train_arrays_path = Path(train_arrays_attr) if train_arrays_attr else None
        decim_hint = getattr(single_dataset, "decim_cache_vtp", None) if single_dataset is not None else None
        decim_cache_vtp = Path(decim_hint) if decim_hint else None
        if decim_cache_vtp is None:
            base_seg_dataset = getattr(single_dataset, "base_dataset", None) if single_dataset is not None else None
            if base_seg_dataset is not None and getattr(base_seg_dataset, "file_paths", None):
                sample_file = Path(base_seg_dataset.file_paths[0])
                target_cells = getattr(base_seg_dataset, "target_cells", None)
                if target_cells is not None:
                    decim_cache_vtp = DECIM_CACHE / f"{sample_file.stem}.c{int(target_cells)}.vtp"
        zscore_mean = self.mean.tolist() if self.mean is not None else None
        zscore_std = self.std.tolist() if self.std is not None else None
        knn_info = {
            "glm1": int(getattr(self.model, "k_short", 6)),
            "glm2": [int(getattr(self.model, "k_short", 6)), int(getattr(self.model, "k_long", 12))],
        }
        if decim_cache_vtp is None or not decim_cache_vtp.exists():
            raise FileNotFoundError(f"Decimated cache mesh not found for {sample_name}: {decim_cache_vtp}")
        if not train_ids_path.exists():
            raise FileNotFoundError(f"Training sample ids missing: {train_ids_path}")
        decim_cache_vtp_str = str(decim_cache_vtp.resolve())
        train_ids_path_str = str(train_ids_path.resolve())
        train_arrays_path_str = str(train_arrays_path.resolve()) if train_arrays_path and train_arrays_path.exists() else None
        fstn_module = getattr(self.model, "fstn", None)
        arch_config = {
            "glm_impl": getattr(self.model, "glm_impl", "edgeconv"),
            "use_feature_stn": bool(fstn_module),
            "k_short": int(getattr(self.model, "k_short", 6)),
            "k_long": int(getattr(self.model, "k_long", 12)),
            "with_dropout": bool(getattr(self.model, "with_dropout", False)),
            "dropout_p": float(getattr(self.model, "dropout_p", 0.0)),
        }

        checkpoint = {
            # æ¨¡å‹æƒé‡
            "state_dict": self.model.state_dict(),
            
            # æ¨¡å‹æ¶æ„ä¿¡æ¯
            "num_classes": self.num_classes,
            "in_channels": 15,  # ç‰¹å¾ç»´åº¦ï¼ˆ9ç‚¹åæ ‡ + 3æ³•å‘ + 3ç›¸å¯¹ä½ç½®ï¼‰
            "arch": arch_config,
            "train_sample_ids_path": train_ids_path_str,
            "train_arrays_path": train_arrays_path_str,

            # å‰å¤„ç† pipeline å¥‘çº¦
            "pipeline": {
                # Z-score æ ‡å‡†åŒ–
                "zscore": {
                    "mean": zscore_mean,
                    "std": zscore_std,
                    "apply": True
                },
                
                # å‡ ä½•é¢„å¤„ç†
                "centered": True,          # ç‰¹å¾æå–å‰å‡è´¨å¿ƒ
                "div_by_diag": True,        # ä½ç½®æŒ‰ç›’å¯¹è§’çº¿å½’ä¸€
                "use_frame": False,        # overfit ä¸ä½¿ç”¨ arch frame
                
                # é‡‡æ ·ç­–ç•¥
                "sampler": "class_balanced",  # overfit ä½¿ç”¨åˆ†å±‚é‡‡æ ·
                "sample_cells": 6000,      # é‡‡æ ·çš„ cell æ•°é‡
                "target_cells": 10000,     # æŠ½å–åçš„ç›®æ ‡ cell æ•°é‡
                
                # ç‰¹å¾å¸ƒå±€ï¼ˆç”¨äºæ¨ç†æ—¶çš„æ—‹è½¬å¯¹é½ï¼‰
                "feature_layout": {
                    "rotate_blocks": [
                        [0, 3],    # v0: ä¸‰è§’å½¢é¡¶ç‚¹0ç›¸å¯¹è´¨å¿ƒ
                        [3, 6],    # v1: ä¸‰è§’å½¢é¡¶ç‚¹1ç›¸å¯¹è´¨å¿ƒ
                        [6, 9],    # v2: ä¸‰è§’å½¢é¡¶ç‚¹2ç›¸å¯¹è´¨å¿ƒ
                        [9, 12],   # normal: æ³•å‘é‡
                        [12, 15]   # cent_rel: cellä¸­å¿ƒç›¸å¯¹è´¨å¿ƒ
                    ]
                },
                
                # éšæœºç§å­ï¼ˆä¾¿äºå¤ç°ï¼‰
                "seed": 42
            },
            
            # è®­ç»ƒä¿¡æ¯
            "training": {
                "sample_name": sample_name,
                "epoch": epoch,
                "best_dsc": dsc,
                "optimizer": "Adam",
                "lr": 0.01,
                "weight_decay": 0.0
            },
            
            # æ ‡ç­¾ä¿¡æ¯ï¼ˆFDI ç¼–ç  â†’ è¿ç»­ç´¢å¼•æ˜ å°„ï¼‰
            "label_mapping": {
                "fdi_labels": [11, 12, 13, 14, 15, 16, 17, 21, 22, 23, 24, 25, 26, 27],
                "num_classes": self.num_classes,
                "background_class": 0
            }
        }
        
        checkpoint["pipeline"].update({
            "decim_cache_vtp": decim_cache_vtp_str,
            "train_ids_path": train_ids_path_str,
            "train_arrays_path": train_arrays_path_str,
            "diag_mode": "cells",
            "zscore_mean": zscore_mean,
            "zscore_std": zscore_std,
            "knn_k": {"to10k": 5, "tofull": 7}, 
            "train_sample_ids_path": train_ids_path_str,
            
        })
        
        checkpoint["training"].update({
            "seed": 42,
            "numpy_rng": 42,
            "torch_seed": 42,
        })
        torch.save(checkpoint, ckpt_path)
        print(f"ğŸ’¾ ä¿å­˜ checkpoint (å« pipeline å¥‘çº¦): {ckpt_path.name}")
    
    def train(self, epochs: int, save_dir: Path, sample_name: str):
        """æ‰§è¡Œè¿‡æ‹Ÿåˆè®­ç»ƒ"""
        print(f"\nğŸš€ å¼€å§‹å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒ:")
        print(f"   - æ ·æœ¬: {sample_name}")
        print(f"   - Epochs: {epochs}")
        print(f"   - è®¾å¤‡: {self.device}")
        print(f"   - å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']}")
        
        save_dir.mkdir(parents=True, exist_ok=True)
        best_dsc = 0.0
        
        for epoch in range(1, epochs + 1):
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # âš¡ ä¼˜åŒ–ï¼šåªåœ¨ç‰¹å®š epoch è¿›è¡Œè¯„ä¼°ï¼ˆå‡å°‘éªŒè¯å¼€é”€ï¼‰
            should_evaluate = (epoch == 1 or epoch % 10 == 0 or epoch == epochs)
            
            if should_evaluate:
                # è¯„ä¼°
                eval_metrics = self.evaluate()
                
                # è®°å½•å†å²
                self.history['loss'].append(train_metrics['loss'])
                self.history['dice_loss'].append(train_metrics['dice_loss'])
                self.history['ce_loss'].append(train_metrics['ce_loss'])
                self.history['dsc'].append(eval_metrics['dsc'])
                self.history['accuracy'].append(train_metrics['accuracy'])
                self.history['train_bg0'].append(train_metrics['bg_ratio'])
                self.history['train_entropy'].append(train_metrics['entropy'])
                self.history['val_bg0'].append(eval_metrics['bg_ratio'])
                self.history['val_entropy'].append(eval_metrics['entropy'])
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŒ…å«å®Œæ•´çš„ pipeline å…ƒæ•°æ®å¥‘çº¦ï¼‰
                if eval_metrics['dsc'] > best_dsc:
                    best_dsc = eval_metrics['dsc']
                    self._save_checkpoint_with_pipeline(
                        save_dir / f"best_overfit_{sample_name}.pt",
                        sample_name,
                        epoch,
                        eval_metrics['dsc']
                    )
                
                # æ‰“å°è¿›åº¦ï¼ˆåŒ…å«æ–°è¯Šæ–­æŒ‡æ ‡ï¼‰
                print(f"Epoch {epoch:3d}/{epochs} | "
                      f"Loss: {train_metrics['loss']:.6f} | "
                      f"DSC: {eval_metrics['dsc']:.4f} | "
                      f"Acc: {train_metrics['accuracy']:.4f} | "
                      f"ğŸ” Train BG0: {train_metrics['bg_ratio']:.3f} | "
                      f"Train Ent: {train_metrics['entropy']:.3f} | "
                      f"Val BG0: {eval_metrics['bg_ratio']:.3f} | "
                      f"Val Ent: {eval_metrics['entropy']:.3f}", flush=True)
            else:
                # å¿«é€Ÿæ¨¡å¼ï¼šåªæ‰“å°è®­ç»ƒæŒ‡æ ‡ï¼Œä¸åšè¯„ä¼°
                if epoch % 5 == 0:  # æ¯ 5 ä¸ª epoch æ‰“å°ä¸€æ¬¡
                    print(f"Epoch {epoch:3d}/{epochs} | "
                          f"Loss: {train_metrics['loss']:.6f} | "
                          f"Acc: {train_metrics['accuracy']:.4f} | "
                          f"ğŸ” BG0: {train_metrics['bg_ratio']:.3f} | "
                          f"Ent: {train_metrics['entropy']:.3f}", flush=True)
        
        print(f"\nâœ… è¿‡æ‹Ÿåˆè®­ç»ƒå®Œæˆ! æœ€ä½³DSC: {best_dsc:.4f}")
        
        # ğŸ”¬ å†³ç­–æ ‘èŠ‚ç‚¹1ï¼šä¿å­˜æœ€ç»ˆ epoch çš„ logits å’Œ labels
        print(f"\nğŸ”¬ ä¿å­˜è®­ç»ƒè¯æ®ï¼ˆå†³ç­–æ ‘èŠ‚ç‚¹1ï¼‰...")
        self._save_training_evidence(save_dir, sample_name)
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_plots(save_dir, sample_name)
        
        return best_dsc
    
    def save_training_plots(self, save_dir: Path, sample_name: str):
        """ä¿å­˜è®­ç»ƒæ›²çº¿å›¾"""
        epochs = range(1, len(self.history['loss']) + 1)
        
        plt.figure(figsize=(20, 12))
        
        # Loss curves
        plt.subplot(3, 3, 1)
        plt.plot(epochs, self.history['loss'], 'b-', label='Total Loss')
        plt.plot(epochs, self.history['dice_loss'], 'r-', label='Dice Loss')
        plt.plot(epochs, self.history['ce_loss'], 'g-', label='CE Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training Loss')
        plt.legend()
        plt.yscale('log')
        
        # DSC curve
        plt.subplot(3, 3, 2)
        plt.plot(epochs, self.history['dsc'], 'b-', label='DSC')
        plt.xlabel('Epoch')
        plt.ylabel('DSC')
        plt.title('Dice Similarity Coefficient')
        plt.legend()
        
        # Accuracy curve
        plt.subplot(3, 3, 3)
        plt.plot(epochs, self.history['accuracy'], 'g-', label='Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('Training Accuracy')
        plt.legend()
        
        # BG0 ratio curves (key diagnostic)
        plt.subplot(3, 3, 4)
        plt.plot(epochs, self.history['train_bg0'], 'r-', label='Train BG0', linewidth=2)
        plt.plot(epochs, self.history['val_bg0'], 'b-', label='Val BG0', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('BG0 Ratio')
        plt.title('Background Prediction Ratio (should drop fast)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ğŸ” Entropy æ›²çº¿ï¼ˆå…³é”®è¯Šæ–­æŒ‡æ ‡ï¼‰
        plt.subplot(3, 3, 5)
        plt.plot(epochs, self.history['train_entropy'], 'r-', label='Train Entropy', linewidth=2)
        plt.plot(epochs, self.history['val_entropy'], 'b-', label='Val Entropy', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('Entropy')
        plt.title('Prediction Entropy (should drop fast)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # DSC vs Lossæ•£ç‚¹å›¾
        plt.subplot(3, 3, 6)
        plt.scatter(self.history['loss'], self.history['dsc'], alpha=0.6)
        plt.xlabel('Total Loss')
        plt.ylabel('DSC')
        plt.title('DSC vs Loss')
        
        # Last 50 epochs loss
        if len(epochs) > 50:
            plt.subplot(3, 3, 7)
            last_50 = epochs[-50:]
            plt.plot(last_50, self.history['loss'][-50:], 'b-', label='Total Loss')
            plt.plot(last_50, self.history['dice_loss'][-50:], 'r-', label='Dice Loss')
            plt.plot(last_50, self.history['ce_loss'][-50:], 'g-', label='CE Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.title('Last 50 Epochs - Loss')
            plt.legend()
            plt.yscale('log')
        
        # Last 50 epochs DSC
        if len(epochs) > 50:
            plt.subplot(3, 3, 8)
            plt.plot(last_50, self.history['dsc'][-50:], 'b-', label='DSC')
            plt.xlabel('Epoch')
            plt.ylabel('DSC')
            plt.title('Last 50 Epochs - DSC')
            plt.legend()
            
            # Last 50 epochs BG0 (key diagnostic)
            plt.subplot(3, 3, 9)
            plt.plot(last_50, self.history['train_bg0'][-50:], 'r-', label='Train BG0')
            plt.plot(last_50, self.history['val_bg0'][-50:], 'b-', label='Val BG0')
            plt.xlabel('Epoch')
            plt.ylabel('BG0 Ratio')
            plt.title('Last 50 Epochs - BG0 Ratio')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.suptitle(f'Single Sample Overfitting - {sample_name}', fontsize=16)
        plt.tight_layout()
        
        plot_file = save_dir / f"overfit_curves_{sample_name}.png"
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_file}")

def _save_overfit_checkpoint(model, ckpt_path: Path, *,
                             num_classes: int,
                             mean: np.ndarray, std: np.ndarray,
                             sample_cells: int = 6000,
                             target_cells: int = 10000,
                             train_ids_path: Path | None,
                             decim_cache_vtp: Path | None,
                             train_arrays_path: Path | None = None):
    fstn_module = getattr(model, "fstn", None)
    arch_config = {
        "glm_impl": getattr(model, "glm_impl", "edgeconv"),
        "use_feature_stn": bool(fstn_module),
        "k_short": int(getattr(model, "k_short", 6)),
        "k_long": int(getattr(model, "k_long", 12)),
        "with_dropout": bool(getattr(model, "with_dropout", False)),
        "dropout_p": float(getattr(model, "dropout_p", 0.0)),
    }
    train_ids_str = str(train_ids_path.resolve()) if train_ids_path else None
    train_arrays_str = str(train_arrays_path.resolve()) if train_arrays_path else None
    decim_cache_str = str(decim_cache_vtp.resolve()) if decim_cache_vtp else None

    payload = {
        "state_dict": model.state_dict(),
        "num_classes": int(num_classes),
        "in_channels": 15,
        "arch": arch_config,
        # å…¼å®¹å­—æ®µï¼ˆè€ç‰ˆæœ¬ä¼šä»è¿™äº›é”®è¯»å–ï¼‰
        "train_sample_ids_path": train_ids_str,
        "train_arrays_path": train_arrays_str,
        # ç»Ÿä¸€å¥‘çº¦ï¼ˆæ¨ç†ç«¯ä¼˜å…ˆä½¿ç”¨è¿™é‡Œçš„å­—æ®µï¼‰
        "pipeline": {
            "zscore": {
                "apply": True,
                "mean": mean.astype(np.float32).tolist(),
                "std":  np.clip(std, 1e-6, None).astype(np.float32).tolist(),
            },
            "centered": True,          # è®­ç»ƒæ—¶ mesh.points -= center
            "div_by_diag": True,       # è®­ç»ƒä½ç½® pos_norm / diag
            "use_frame": False,        # è‹¥åç»­æä¾› arch frame å¯åˆ‡ True
            "sampler": "random",       # è®­ç»ƒ 6k éšæœºé‡‡æ ·ï¼ˆæ¨ç†å¤ç”¨ train_idsï¼‰
            "sample_cells": int(sample_cells),
            "target_cells": int(target_cells),
            "train_ids_path": train_ids_str,
            "train_arrays_path": train_arrays_str,
            "decim_cache_vtp": decim_cache_str,
            # è®°å½• knn kï¼ˆä¾›åå¤„ç†å…œåº•ï¼‰
            "knn_k": {"to10k": 5, "tofull": 7},
            "diag_mode": "cells",
            "seed": 42,
        },
    }
    ckpt_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(payload, str(ckpt_path))
    print(f"ğŸ’¾ ä¿å­˜ checkpoint (å« pipeline å¥‘çº¦): {ckpt_path.name}")
    
def main():
    parser = argparse.ArgumentParser(description="å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒ")
    parser.add_argument("--sample", type=str, required=True, 
                       help="æ ·æœ¬åç§°ï¼Œå¦‚ 1_L æˆ– 5_U")
    parser.add_argument("--epochs", type=int, default=300,
                       help="è®­ç»ƒepochæ•° (é»˜è®¤: 300)")
    parser.add_argument("--dataset-root", type=str, default="datasets/segmentation_dataset",
                       help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--device", type=str, default="auto",
                       help="è®¾å¤‡ (cuda/cpu/auto)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¯ iMeshSegNet å•æ ·æœ¬è¿‡æ‹Ÿåˆè®­ç»ƒ")
    print("=" * 60)
    print(f"æ ·æœ¬: {args.sample}")
    print(f"Epochs: {args.epochs}")
    print(f"æ•°æ®é›†: {args.dataset_root}")
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    print(f"è®¾å¤‡: {device}")
    
    # è®¾ç½®æ•°æ®
    dataset_root = Path(args.dataset_root)
    dataloader, num_classes, mean, std = setup_single_sample_training(args.sample, dataset_root)
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹ (ç±»åˆ«æ•°: {num_classes})")
    model = iMeshSegNet(num_classes=num_classes, with_dropout=False, use_feature_stn=False)
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ å…¥ mean, std ç”¨äºä¿å­˜ pipeline å¥‘çº¦ï¼‰
    trainer = OverfitTrainer(model, dataloader, num_classes, device, mean, std)
    
    # è¾“å‡ºç›®å½•
    output_dir = Path("outputs/segmentation/overfit") / args.sample
    
    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    best_dsc = trainer.train(args.epochs, output_dir, args.sample)
    end_time = time.time()
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"   - æœ€ä½³DSC: {best_dsc:.4f}")
    print(f"   - è®­ç»ƒæ—¶é—´: {end_time - start_time:.1f}ç§’")
    print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    # æœŸæœ›ç»“æœåˆ†æ
    print(f"\nğŸ“ˆ ç»“æœåˆ†æ:")
    if best_dsc > 0.95:
        print(f"   âœ… ä¼˜ç§€! DSC > 0.95ï¼Œæ¨¡å‹èƒ½å¤Ÿå®Œç¾è¿‡æ‹Ÿåˆå•æ ·æœ¬")
    elif best_dsc > 0.8:
        print(f"   âœ… è‰¯å¥½! DSC > 0.8ï¼Œæ¨¡å‹åŸºæœ¬æ­£å¸¸ï¼Œå¯èƒ½éœ€è¦æ›´å¤šepoch")
    elif best_dsc > 0.5:
        print(f"   âš ï¸  ä¸€èˆ¬! DSC > 0.5ï¼Œæ¨¡å‹å­¦ä¹ èƒ½åŠ›æœ‰é™ï¼Œæ£€æŸ¥æ•°æ®æˆ–æ¨¡å‹ç»“æ„")
    else:
        print(f"   âŒ å¼‚å¸¸! DSC < 0.5ï¼Œå­˜åœ¨ä¸¥é‡é—®é¢˜:")
        print(f"      - æ£€æŸ¥æ•°æ®æ ¼å¼å’Œæ ‡ç­¾æ˜ å°„")
        print(f"      - æ£€æŸ¥æ¨¡å‹è¾“å…¥è¾“å‡ºç»´åº¦")
        print(f"      - æ£€æŸ¥æŸå¤±å‡½æ•°é…ç½®")


if __name__ == "__main__":
    main()


