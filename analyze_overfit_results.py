"""
è¯¦ç»†åˆ†æ overfit è®­ç»ƒå’Œæ¨ç†ç»“æœ
"""
import numpy as np
import pyvista as pv

print("=" * 70)
print("ğŸ“Š Overfit è®­ç»ƒä¸æ¨ç†ç»“æœåˆ†æ")
print("=" * 70)

# 1. åŠ è½½è®­ç»ƒæ•°æ®çš„çœŸå®æ ‡ç­¾åˆ†å¸ƒ
print("\n1ï¸âƒ£  è®­ç»ƒæ•°æ®ï¼ˆ1_U.vtpï¼‰çœŸå®æ ‡ç­¾åˆ†å¸ƒ:")
print("-" * 70)
train_mesh = pv.read('datasets/segmentation_dataset/1_U.vtp')
train_labels = train_mesh.cell_data['Label']

# æ˜ å°„ FDI åˆ°è¿ç»­ç´¢å¼•
from src.iMeshSegNet.m0_dataset import remap_segmentation_labels
train_labels_mapped = remap_segmentation_labels(np.asarray(train_labels))

unique_train, counts_train = np.unique(train_labels_mapped, return_counts=True)
total_train = len(train_labels_mapped)

for u, c in zip(unique_train, counts_train):
    percentage = c / total_train * 100
    bar = 'â–ˆ' * int(percentage / 2)  # å¯è§†åŒ–æ¡å½¢å›¾
    print(f"  L{u:2d}: {c:6d} cells ({percentage:5.1f}%) {bar}")

print(f"\n  æ€»è®¡: {total_train:,} cells, {len(unique_train)} ä¸ªç±»åˆ«")

# 2. æ¨ç†ç»“æœæ ‡ç­¾åˆ†å¸ƒ
print("\n2ï¸âƒ£  æ¨ç†ç»“æœï¼ˆ1_U_colored.vtpï¼‰æ ‡ç­¾åˆ†å¸ƒ:")
print("-" * 70)
pred_labels = np.load('outputs/overfit/infer/1_U_pred.npy')
unique_pred, counts_pred = np.unique(pred_labels, return_counts=True)
total_pred = len(pred_labels)

for u, c in zip(unique_pred, counts_pred):
    percentage = c / total_pred * 100
    bar = 'â–ˆ' * int(percentage / 2)
    print(f"  L{u:2d}: {c:6d} cells ({percentage:5.1f}%) {bar}")

print(f"\n  æ€»è®¡: {total_pred:,} cells, {len(unique_pred)} ä¸ªç±»åˆ«")

# 3. å¯¹æ¯”åˆ†æ
print("\n3ï¸âƒ£  å¯¹æ¯”åˆ†æ:")
print("-" * 70)

missing = set(unique_train) - set(unique_pred)
extra = set(unique_pred) - set(unique_train)

if missing:
    print(f"âŒ é—æ¼çš„ç±»åˆ«: {sorted(missing)}")
    print(f"   è¿™äº›ç±»åˆ«åœ¨è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨ï¼Œä½†æ¨¡å‹æ²¡æœ‰é¢„æµ‹å‡ºæ¥")
else:
    print("âœ… æ²¡æœ‰é—æ¼ç±»åˆ«ï¼")

if extra:
    print(f"âš ï¸  å¤šä½™çš„ç±»åˆ«: {sorted(extra)}")
    print(f"   è¿™äº›ç±»åˆ«åœ¨è®­ç»ƒæ•°æ®ä¸­ä¸å­˜åœ¨ï¼Œä½†æ¨¡å‹é¢„æµ‹äº†")

# 4. ç±»åˆ«çº§åˆ«å¯¹æ¯”
print("\n4ï¸âƒ£  å„ç±»åˆ«é¢„æµ‹å‡†ç¡®æ€§ï¼ˆè®­ç»ƒ vs æ¨ç†ï¼‰:")
print("-" * 70)
print(f"{'ç±»åˆ«':<6} {'è®­ç»ƒæ•°æ®%':>12} {'æ¨ç†ç»“æœ%':>12} {'å·®å¼‚':>10} {'çŠ¶æ€':<10}")
print("-" * 70)

# åˆ›å»ºå®Œæ•´çš„ç±»åˆ«åˆ—è¡¨ï¼ˆ0-14ï¼‰
all_labels = set(range(15))
train_dist = {u: c/total_train*100 for u, c in zip(unique_train, counts_train)}
pred_dist = {u: c/total_pred*100 for u, c in zip(unique_pred, counts_pred)}

for label in sorted(all_labels):
    train_pct = train_dist.get(label, 0.0)
    pred_pct = pred_dist.get(label, 0.0)
    diff = pred_pct - train_pct
    
    if label not in pred_dist:
        status = "âŒ é—æ¼"
    elif abs(diff) > 20:
        status = "âš ï¸  åå·®å¤§"
    elif abs(diff) > 10:
        status = "âš ï¸  åå·®ä¸­"
    else:
        status = "âœ… æ­£å¸¸"
    
    print(f"L{label:2d}     {train_pct:>10.1f}%  {pred_pct:>10.1f}%  {diff:>+9.1f}%  {status}")

# 5. æ€»ç»“
print("\n5ï¸âƒ£  æ€»ç»“:")
print("-" * 70)

# è®¡ç®—èƒŒæ™¯ç±»åˆ«çš„å˜åŒ–
bg_train = train_dist.get(0, 0)
bg_pred = pred_dist.get(0, 0)

print(f"èƒŒæ™¯ç±»åˆ« (L0):")
print(f"  è®­ç»ƒæ•°æ®: {bg_train:.1f}%")
print(f"  æ¨ç†ç»“æœ: {bg_pred:.1f}%")
print(f"  å·®å¼‚: {bg_pred - bg_train:+.1f}%")

if bg_pred > bg_train + 10:
    print(f"  âš ï¸  èƒŒæ™¯æ¯”ä¾‹è¿‡é«˜ï¼Œæ¨¡å‹å°†è®¸å¤šç‰™é½¿é¢„æµ‹ä¸ºèƒŒæ™¯")

# è®¡ç®—è¦†ç›–ç‡
coverage = len(unique_pred) / len(unique_train) * 100
print(f"\nç±»åˆ«è¦†ç›–ç‡: {coverage:.1f}% ({len(unique_pred)}/{len(unique_train)})")

if coverage < 100:
    print(f"  âŒ æ¨¡å‹æœªèƒ½å­¦ä¹ æ‰€æœ‰ç±»åˆ«")
    print(f"  å»ºè®®:")
    print(f"    1. å¢åŠ è®­ç»ƒè½®æ•°åˆ° 100-200 epochs")
    print(f"    2. è°ƒæ•´å­¦ä¹ ç‡æˆ–æŸå¤±å‡½æ•°æƒé‡")
    print(f"    3. æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜")
else:
    print(f"  âœ… æ¨¡å‹æˆåŠŸå­¦ä¹ äº†æ‰€æœ‰ç±»åˆ«ï¼")

print("\n" + "=" * 70)
print("ğŸ“ˆ è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜: outputs/overfit/1_U/overfit_curves_1_U.png")
print("ğŸ¨ æ¨ç†ç»“æœå·²ä¿å­˜: outputs/overfit/infer/1_U_colored.vtp")
print("=" * 70)
